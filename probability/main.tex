\documentclass[a4paper]{article}
\usepackage{parskip}
\usepackage{lipsum}
\usepackage{newtxtext}
\usepackage{eucal}

\def\nterm {Spring}
\def\nyear {2026}
\def\ncourse {Probability}

\input{../header}
\author{Notes taken by \nauthor\vspace{5pt}\\
Lectures by Konstantin Tikhomirov\vspace{5pt}\\
Carnegie Mellon University}

\newcommand{\TODO}{\textcolor{red}{\textbf{*** TO-DO ***}}}

\begin{document}
\maketitle

\tableofcontents

\section{Measure theory review}

\subsection{Measurable space and mapping}

\begin{defi}[$\sigma$-field]
  A collection of subsets $\Sigma \subset 2^{\Omega}$ is a $\sigma$-field if
  \begin{itemize}
    \item $\emptyset \in \Sigma$.
    \item If $A \in \Sigma$, then $A^c \in \Sigma$.
    \item If $\seqinfi{A_i} \subset \Sigma$, then $\cupinfi A_i \in \Sigma$.
  \end{itemize}
  The pair $(\Omega, \Sigma)$ is called a measurable space.
\end{defi}

\begin{defi}[atom]
Let $\Sigma$ be a $\sigma$-field. Say $A \in \Sigma$ is an atom
if for all $B \in \Sigma$ either $A \subset B$ or $A \cap B = \emptyset$.
\end{defi}

\begin{prop}
For all $\omega \in \Omega$, there exists atom $A \in \Sigma$ containing
$\omega$ if $\Omega$ is finite or countable.
\end{prop}

\begin{proof}
  Define $\tilde{A} = \bigcap \left\{ B \in \Sigma :
  \omega \in B \right\}$. We can check that $\tilde{A} \in \Sigma$
  and $\tilde{A}$ is an atom containing $\omega$.
\end{proof}

\begin{cor}
If $\Omega$ is finite or countable, there exists a partition
$\Omega = \bigsqcup_i \Omega_i$,
where each $\Omega_i$ is an atom of $\Sigma$. With this partition,
$\Sigma$ is just the power set with respect to $\left\{ \Omega_i \right\}_i$.
\end{cor}

\begin{defi}
If $F \subset 2^\Omega$, then the $\sigma$-field generated by $F$ is the
smallest $\sigma$-field containing all elements of $F$.
Write this $\sigma$-field as $\sigma (F)$.
\end{defi}

\begin{eg}
Let $\Omega = \left\{ 1, 2, 3, 4, 5 \right\}$ and
$F = \left\{ \left\{ 2,3 \right\}, \left\{ 3,4 \right\} \right\}$.
Construct $\sigma$-field $\Sigma$ generated by $F$.
$\Sigma$ is all possible union of sets from the collection
$\left\{ \{2\}, \{3\}, \{4\}, \{1, 5\}\right\}$.
\end{eg}

\begin{defi}[measurable mapping]
Given two measurable spaces $(\Omega, \Sigma)$ and $(\tilde{\Omega},
\tilde{\Sigma})$. Then $f : \Omega \to \tilde{\Omega}$ is measurable
if $f^{-1} (B) \in \Sigma$ for all $B \in \tilde{\Sigma}$.
\end{defi}

\begin{defi}[Borel $\sigma$-field]
Let $(T, \tau)$ be a topological space. Then the Borel $\sigma$-field
$\B(T, \tau)$ is defined as the smallest $\sigma$-field containing
all open sets.
\end{defi}

\begin{defi}[product measurable space]
Given two measurable spaces $(\Omega, \Sigma)$ and
$(\tilde{\Omega}, \tilde{\Sigma})$. We can define the product
measurable space as follows: let the ground set be
$\Omega \times \tilde{\Omega}$, and let
$\Sigma \otimes \tilde{\Sigma}$ be the smallest $\sigma$-field
containing all rectangles $B \times \tilde{B}$ where
$B \in \Sigma$ and $\tilde{B} \in \tilde{\Sigma}$.

More generally,
let $\Lambda$ be an index set and $(\Omega_\lambda, \Sigma_\lambda)_{\lambda
\in \Lambda}$. Define the product $\sigma$-field $\bigotimes_{\lambda
\in \Lambda} \Sigma_\lambda$ be
the smallest $\sigma$-field containing all elements
in the form of $\prod_{\lambda \in \Lambda} B_\lambda$ where
$B_\lambda \in \Sigma_\lambda$ and $B_\lambda = \Omega_\lambda$ for all
but countably many indices.
\end{defi}

\begin{prop}
  Let $\left( \Omega_i, \Sigma_i \right)_{i=1}^n$ be measurable
  spaces and $\left( \prod_{i=1}^n \Omega_i, \bigotimes_{i=1}^n \Sigma_i
   \right)$ be the product space. Let $(\Omega, \Sigma)$ be the domain
  and $f = (f_1, \dots, f_n) : (\Omega, \Sigma) \to (\prod_{i=1}^n
  \Omega_i, \bigotimes_{i=1}^n \Sigma_i)$. Suppose $f$ is measurable,
  then every coordinate projection $f_i : \Omega \to \Omega_i$ is
  measurable.

  This is also true for arbitrary index set.
\end{prop}

\begin{prop}
  If $f : (\Omega, \Sigma) \to (\Omega_f, \Sigma_f)$ and
  $g : (\Omega, \Sigma) \to (\Omega_g, \Sigma_g)$, then the concatenation
  $(f, g)$ is measurable w.r.t. the product space
  $(\Omega_f \times \Omega_g, \Sigma_f \otimes \Sigma_g)$.
\end{prop}

\begin{proof}
  Let $A \times B$ be such that $A \in \Sigma_f$ and $B \in \Sigma_g$.
  Then the preimage
  \[
  (f, g)^{-1} (A \times B) = f^{-1} (A) \cap g^{-1} (B) \in \Sigma.
  \]
  By definition, the product $\sigma$-field is generated by rectangles,
  so the proof is complete.
\end{proof}

\subsection{Measure space}
\begin{defi}[measure]
  Let $(\Omega, \Sigma)$ be a measurable space. Then $\mu : \Sigma \to
  [0, \infty]$ is a measure if
  \begin{itemize}
    \item $\mu (\emptyset) = 0$.
    \item If $A_i \in \Sigma$ is pairwise disjoint then
    $\mu \left( \cupinfi A_i \right) = \suminfi \mu (A_i)$.
  \end{itemize}
\end{defi}

\begin{prop}[continuity of measure]
  If $A_1 \subset A_2 \subset \dots$ is a nested sequence of elements
  of $\Sigma$ and $\mu$ be any measure on $(\Omega, \Sigma)$. Then
  \[
  \mu \left( \cupinfi A_i \right) = \lim_{i \to \infty} \mu (A_i).
  \]

  If $A_1 \supset A_2 \supset \dots$ is a nested sequence of elements
  of $\Sigma$ and $\mu (A_n) < \infty$ for some $n$. Then
  \[
  \mu \left( \capinfi A_i \right) = \lim_{i \to \infty} \mu (A_i).
  \]
\end{prop}

\begin{defi}
  Let $(\Omega, \Sigma, \mu)$ be a measure space.

  Say $\mu$ is $\sigma$-finite
  if there is a representation $\Omega = \cupinfi \Omega_i$ where
  $\Omega_i \in \Sigma$ and $\mu (\Omega_i) < \infty$.

  Say $\mu$ is a probability measure if $\mu(\Omega) = 1$.
\end{defi}

\begin{defi}[completion of measure space]
  Let $(\Omega, \Sigma, \mu)$ be a measure space. Let
  \[
  \tilde{\Sigma} = \left\{ A \cup B : A \in \Sigma,
  B \subset \Omega, \text{there exists $C \in \Sigma$ with
  $\mu(C) = 0$ and $B \subset C$} \right\}.
  \]
  We can check $\tilde{\Sigma}$ is a $\sigma$-field. If $\tilde{\mu}$
  is a measure on $(\Omega, \tilde{\Sigma})$ which agrees with $\mu$
  on $\Sigma$, then $(\Omega, \tilde{\Sigma}, \tilde{\mu})$ is called
  a completion of $(\Omega, \Sigma, \mu)$.
\end{defi}

\subsection{$\pi$-$\lambda$ theorem}

\begin{defi}[$\pi$-system]
  Let $\Omega$ be a set and $\P$ be a collection of subsets of $\Omega$.
  Then $\P$ is a $\pi$-system if it is closed with respect to
  taking finite intersections. That is, $A, B \in \P$ implies $A \cap B
  \in \P$.
\end{defi}

\begin{eg}
  On the real line $\R$, both $\P_1 = \left\{ (a, b) : a < b \right\}$
  and $\P_2 = \left\{ (-\infty, a] : a \in \R \right\}$ are
  $\pi$-systems.
\end{eg}

\begin{defi}[$\lambda$-system]
  Let $\Omega$ be a set and $\L$ be a collection of subsets of $\Omega$.
  Say $\L$ is a $\lambda$-system if
  \begin{itemize}
    \item $\emptyset \in \L$.
    \item $A \in \L$ implies $A^c \in \L$.
    \item for all countable collection of disjoint elements $A_i \in \L$,
    we have $\cupinfi A_i \in \L$.
  \end{itemize}
\end{defi}

\begin{thm}[$\pi$-$\lambda$ theorem]
  Let $\Omega$ be a set, $\P$ be a $\pi$-system and $\L$ be a $\lambda$-system.
  Also suppose $\P \subset \L$, then $\sigma (\P) \subset \L$.
\end{thm}

\begin{proof}
  Let $\ell (\P)$ be the smallest $\lambda$-system on $\Omega$ containing
  $\P$. The goal is to show that $\ell (\P)$ is a $\sigma$-field.
  We need to show that if $A_i \in \ell (\P)$ for $1 \leq i < \infty$,
  then $\cupinfi A_i \in \ell (\P)$. Note that
  \[
  \cupinfi A_i = \cupinfi \left( A_i \setminus \cupj^{i - 1} A_i \right),
  \]
  so it suffices to show that $A, B \in \ell (\P)$ implies $A \cap B \in
  \ell (\P)$.

  For $A \in \ell(\P)$ we define
  \[
  W_A = \left\{ B \subset \Omega : A \cap B \in \ell (\P) \right\}.
  \]
  It can be directly verified that $W_A$ is a $\lambda$-system.

  Take $A \in \P$, then for any $B \in \P$ we have $A \cap B \in \P
  \subset \ell (\P)$. Hence, $\P \subset W_A$ and thus $\ell (\P) \subset W_A$
  for all $A \in \P$, as $\ell (\P)$ is the smallest $\lambda$-system on
  $\Omega$ containing $\P$.
  Now take $A \in \ell (\P)$, we have $A \in W_B$
  for all $B \in \P$. It follows that $A \cap B \in \ell (\P)$ and thus
  $B \in W_A$. Hence similarly
  $\ell (\P) \subset W_A$ for all $A \in \ell (\P)$.

  Now for any pair $B, C \in \ell (\P)$, we have $C \in W_B$ and thus
  $B \cap C \in \ell (\P)$. This completes the proof.
\end{proof}

\subsection{Extension theorems}

\begin{defi}[semi-field]
  A collection of subsets $S \subset 2^{\Omega}$ is a semi-field if
  \begin{itemize}
    \item $\emptyset \in S$ and $\Omega \in S$.
    \item $A, B \in S$ implies $A \cap B \in S$.
    \item If $A \in S$, then $A^c$ is a finite disjoint union of sets in $S$.
  \end{itemize}
\end{defi}

\begin{thm}[Caratheorody's extension theorem]
  Let $S$ be a semi-field and let $\mu$ be a non-negative function on $S$
  satisfying:
  \begin{itemize}
    \item $\mu (\emptyset) = 0$.
    \item If $A_1, \dots, A_n$ are disjoint and $\cupi^n A_i \in S$, then
    $\mu (\cupi^n A_i) = \sumi^n \mu (A_i)$.
    \item If $A_1, A_2, \dots$ are such that $\cupinfi A_i \in S$, then
    $\mu (\cupinfi A_i) \leq \suminfi \mu (A_i)$.
  \end{itemize}
  Then $\mu$ admits a unique extension $\bar{\mu}$ which is a
  measure on $\bar{S}$, the field (algebra) generated by $S$. Moreover,
  if $\bar{\mu}$ is $\sigma$-finite then $\bar{\mu}$ admits a unique
  extension $\tilde{\mu}$ to $\sigma (S)$.
\end{thm}

\begin{notation}
  Let $T$ be any set. Write
  \[
  \R^T = \left\{ \left( \omega_t \right)_{t \in T} :
  \omega_t \in \R \right\}.
  \]
  Also write $\rcal^T$ as the $\sigma$-field generated by rectangles
  of the form $\prod_{t \in T} I_t$, where for each $t \in T$,
  $I_t$ is either a semi-open interval of the form $(a, b]$
  with $a < b$ or $I_t = \R$, and $I_t = \R$ for all but finitely
  many $t \in T$.
\end{notation}

\begin{thm}[Kolmogorov's extension theorem]
  For each finite non-empty subset $J \subset T$,
  let $\mu_J$ be a Borel probability measure in $\R^J$,
  and assume that the measures $\left( \mu_J \right)_{J \subset T,
  \abs{J} < \infty}$ are
  compatible, in the sense that whenever $J_1 \subset J_2 \subset T$
  with $0 \leq \abs{J_1} \leq \abs{J_2} < \infty$,
  $I_j \subset \R$ with $j \in J_1$ are Borel subsets of $\R$, and
  \[
  \tilde{I_j} = \begin{cases}
    I_j & (j \in J_1) \\
    \R & (j \in J_2 \setminus J_1),
  \end{cases}
  \]
  one has
  \[
  \mu_{J_2} \left( \prod_{j \in J_2} \tilde{I_{j}} \right)
  = \mu_{J_1} \left( \prod_{j \in J_1} I_j \right).
  \]
  Then there exists a unique probability measure $\mu$ on
  $(\R^T, \rcal^T)$ consistent with $\left( \mu_J \right)_{J \subset T,
  \abs{J} < \infty}$. That is, one has
  \[
  \mu \left( \prod_{t \in T} I_t \right)
  = \mu_J \left( \prod_{j \in J} I_j \right)
  \]
  whenever $J \subset T$ with $\abs{J} < \infty$ and $I_t = \R$
  for all $t \notin J$.
\end{thm}

\subsection{Lebesgue Integration}

Here we provide a proof for dominated convergence theorem that uses
the truncation technique, which will be a useful technique later
in the course.

\begin{thm}[dominated convergence theorem]
  Let $\seqinfn{f_n}$ be a sequence of measurable
  functions on $(\Omega, \Sigma, \mu)$ and $g \geq 0$ be another measurable
  function. Suppose
  \begin{enumerate}
    \item $\int g \d \mu < \infty$.
    \item $\abs{f_n} (\omega) \leq g (\omega)$ for all $\omega \in \Omega$
    and $n \geq 1$.
    \item $f_n \to f$ pointwise.
  \end{enumerate}
  Then
  \[
  \lim_{n \to \infty} \int f_n \d \mu = \int f \d \mu.
  \]
\end{thm}

\begin{proof}
  \textbf{Claim 1.}
  If $h$ is a function on $(\Omega, \Sigma, \mu)$ with
  $h \geq 0$ and $\int h \d \mu < \infty$. Let $\seqinfn{A_n}$
  be any sequence of elements of $\Sigma$ with $\mu (A_n) \to 0$.
  Then
  \[
  \int_{A_n} h \d \mu \to 0.
  \]
  Proof of claim. WLOG assume $\mu(A_n) \leq 2^{-n}$ for all $n$.
  Define $h_n = h \ind_{\bigcup_{i=n}^\infty A_i}$. We then have
  \begin{enumerate}
    \item The sequence $\seqinfn{h_n}$ is monotone.
    \item $h_n$ converges to $0$ almost everywhere.
  \end{enumerate}
  Monotone convergence theorem then implies $\lim_{n \to \infty}
  \int h_n \d \mu = 0$. Meanwhile,
  \[
  0 \leq \int_{A_n} h \d \mu \leq \int h_n \d \mu,
  \]
  and the proof is complete.

  \textbf{Claim 2.}
  Suppose $h \geq 0$ and $\int h \d \mu < \infty$.
  Let $\seqinfn{\epsilon_n}$ be a sequence of strictly positive numbers
  converging to zero. Define
  \[
  B_n = \left\{ \omega \in \Omega : h (\omega) \leq \epsilon_n \right\}
  \in \Sigma.
  \]
  Then
  \[
  \int_{B_n} h \d \mu \to 0.
  \]
  Proof of this claim is left as an exercise.

  Now we prove the theorem. Fix $\epsilon > 0$. By the previous two
  claims, there exists $M > 0$ and $\delta > 0$ such that
  \[
  \int_{\left\{ g \geq M \right\}} g \d \mu < \epsilon,
  \quad
  \int_{\left\{ g \leq \delta \right\}} g \d \mu < \epsilon.
  \]
  Let $U = \left\{ \omega : \delta < g(\omega) < M \right\}$.
  Since $g$ is integrable, $\mu (U) < \infty$.
  For $\omega \in U$, let $n_\epsilon (\omega)$ be the smallest
  index such that $n \geq n_\epsilon (\omega)$ implies
  $\abs{f_n (\omega) - f(\omega)} \leq \epsilon \mu(U)^{-1}$.
  It follows that there exists $N$ such that
  \[
  \mu \left( \left\{ \omega \in U: n_\epsilon (\omega) > N \right\}  \right)
  \leq \frac{\epsilon}{M}.
  \]
  Then, for $n \geq N$, we have
  \begin{align*}
    \abs{\int_U (f_n - f) \d \mu}
    \leq \int_{n_\epsilon (\omega) \leq N} \abs{f_n - f} \d \mu
    + \int_{n_\epsilon (\omega) > N} \abs{f_n - f} \d \mu
    \leq 3 \epsilon.
  \end{align*}
  Now for $n \geq N$, we have
  \[
  \abs{\int (f_n - f) \d \mu}
  \leq 3 \epsilon + \int_{U^c} \abs{f - f_n} \d \mu
  \leq 3 \epsilon + 2 \int_{U^c} g \d \mu \leq 7 \epsilon.
  \]
\end{proof}

\begin{thm}[Markov-Chebyshev inequality]
  Suppose we have probability measure space $(\Omega, \Sigma, \Pr)$
  and $f \geq 0$. Suppose also $\int f \d \Pr < \infty$. Then
  \[
  \Pr \left( \left\{ \omega : f(\omega) > t \right\} \right)
  \leq \frac{1}{t} \int f \d \Pr.
  \]
  for all $t > 0$.
\end{thm}

\begin{remark}
  Let $1 \leq p < \infty$. Suppose $f : (\Omega, \Sigma, \Pr) \to 
  [0, \infty]$ and $\int f^p \d \Pr < \infty$. Then 
  \[
  \Pr \left( \left\{ \omega : f (\omega) > t \right\} \right) 
  \leq \frac{1}{t^p} \int f^p \d \Pr.
  \]
  for all $t > 0$.
\end{remark}

\begin{remark}
  Suppose $\int e^{\lambda f} \d \Pr < \infty$ for all 
  $\lambda \in \R$ and $f : (\Omega, \Sigma, \Pr) \to [0, \infty]$.
  Then 
  \[
  \Pr \left( \left\{ \omega : f (\omega) > t \right\} \right) 
  \leq \frac{1}{e^{\lambda t}} \int e^{\lambda f} \d \Pr
  \]
  for all $t > 0$ and $\lambda > 0$.
\end{remark}

\begin{thm}[H\"older inequality]
  Let $p, q \in [1, \infty]$ and $p^{-1} + q^{-1} = 1$. 
  Let $(\Omega, \Sigma, \mu)$ be a probability space. For any 
  measurable functions $f, g$, we have 
  \[
  \int \abs{f g} \d \Pr \leq \left( \int \abs{f}^p \d \Pr \right)^{1 / p}
  \left( \int \abs{g}^q \d \Pr \right)^{1 / q}.
  \]
\end{thm}

\begin{thm}[Jensen's inequality]
  Let $(\Omega, \Sigma, \mu)$ be a probability space and $f$ be
  integrable. Let $\phi : \bar{\R} \to \bar{\R}$ be convex 
  and suppose $\phi (\infty) = \lim_{x \to \infty} \phi(x)$
  and $\phi (-\infty) = \lim_{x \to -\infty} \phi(x)$. Then 
  \[
  \phi \left( \int f \d \Pr \right) \leq \int \phi (f) \d \Pr.
  \]
\end{thm}

\subsection{Product measures and Fubini theorem}
Let $(\Omega_1, \Sigma_1, \mu_1)$, $(\Omega_2, \Sigma_2, \mu_2)$
be $\sigma$-finite measure spaces. We already defined the product 
$\Sigma_1 \otimes \Sigma_2$. To define a product measure, we first 
consider the algebra of rectangles 
\[
S = \left\{ A \in \Sigma_1 \otimes \Sigma_2 : A = A_1 \times A_2 
\text{ for some $A_1 \in \Sigma_1, A_2 \in \Sigma_2$} \right\}.
\]
Then we can define $\mu = \mu_1 \otimes \mu_2$ on $S$ by 
\[
\mu (A) = \mu_1 (A_1) \mu_2 (A_2)
\]
for $A = A_1 \times A_2$. We can check that the definition is 
self-consistent. That is, if $A = A_1 \times A_2$ is a countable 
union of disjoint rectangles
$\seqinfj{A_1^{(j)} \times A_2^{(j)}}$, we have 
\[
\mu (A_1 \times A_2) = \suminfj \mu ( A_1^{(j)} \times A_2^{(j)} ).
\]
This can be verified with monotone convergence theorem.
Now $\mu$ is a premeasure and can be uniquely 
extended to $\Sigma_1 \otimes \Sigma_2$.

\begin{thm}[Fubini-Tonelli]
  Let $(\Omega_1, \Sigma_1, \mu_1)$, $(\Omega_2, \Sigma_2, \mu_2)$
  be $\sigma$-finite measure spaces and let $(\Omega, \Sigma, \mu)$
  be the product space. Suppose $f$ is measurable on the product space.
  Suppose either $f$ is non-negative or $\int_\Omega \abs{f} \d \mu <
  \infty$. Then 
  \begin{itemize}
    \item $y \mapsto f(x, y)$ is $\Sigma_2$ measurable for all $x \in \Omega_1$.
    \item $x \mapsto \int_{\Omega_2} f(x, y) \d \mu_2 (y)$ is 
    $\Sigma_1$ measurable.
    \item We have
    \[
    \int_{\Omega_1} \int_{\Omega_2} f(x, y) \d \mu_2 (y) \d \mu_1 (x)
    = \int_\Omega f(x, y) \d \mu(x, y).
    \]
  \end{itemize}
\end{thm}

\begin{proof}
  First suppose $f = \ind_{A}$ for $A \in \Sigma$. 
  Also suppose $\mu_1, \mu_2$ are finite. Define section 
  \[
  A_x = \left\{ y \in \Omega_2 : (x, y) \in A \right\}.
  \]
  The goal is to show that $A_x \in \Sigma_2$ for all $x \in \Omega_1$.
  Define a family of sets 
  \[
  \fcal_x = \left\{ B \in \Sigma : \text{$B_x$ is $\Sigma_2$-measurable} \right\}.
  \]
  It can be verified that $\fcal_x$ is a $\sigma$-field for all $x \in \Omega_1$.
  Also, $\fcal_x$ contains all rectangles and thus $\Sigma \subset \fcal_x$.
  Hence, we have shown that $y \mapsto \ind_A (x, y) = \ind_{A_x} (y)$
  is measurable for all $x \in \Omega_1$.

  Next we show $x \mapsto \mu_2 (A_x)$ is measurable 
  and its integral over $\Omega_1$ is equal to $\mu (A)$.
  Define 
  \[
  \ucal = \left\{ B \in \Sigma : \text{$x \mapsto \mu_2(B_x)$ is 
  $\Sigma_1$-measurable and $\int_{\Omega_1} \mu_2(B_x) \d \mu_1 = \mu (B)$} \right\}
  \]
  It can be verified that $\ucal$ is a $\lambda$-system. 
  Note that $\ucal$ also contains all rectangles in $\Sigma$.
  It follows that $\ucal = \Sigma$ and the proof for 
  indicator functions are complete.

  Then use linearity to extend to simple functions, and use monotone
  convergence theorem to prove the statement for non-negative functions.
  For the case where $f$ is integrable, consider the positive and 
  negative part about $f$ to complete the proof.
\end{proof}

\section{Probability theory basics}

\subsection{Distributions and densities}

\begin{defi}
  Let $F : \R \to [0, 1]$. Suppose $F$ is
  \begin{itemize}
    \item right-continuous.
    \item non-decreasing.
    \item $\lim_{t \to -\infty} F(t) = 0$ and 
    $\lim_{t \to \infty} f(t) = 1$.
  \end{itemize}
  Then $F$ is a cumulative distribution function (CDF).
\end{defi}

\begin{remark}
  If we want to define CDF in $\R^2$ then the axioms are 
  \begin{itemize}
    \item right-continuous: $F(\tilde{s}, \tilde{t})
    \to F(s, t)$ as $t \downarrow \tilde{t}$ and 
    $s \downarrow \tilde{s}$.
    \item coordinate-wise non-decreasing.
    \item $\lim_{s, t \to \infty} F(s, t) = 1$, 
    $\lim_{s \to -\infty} F(s, t) = 0$ for any $t$, and 
    $\lim_{t \to - \infty} F (s, t) = 0$ for any $s$.
    \item For a rectangle with bottom left vertex $(a_1, a_2)$
    and top right vertex $(b_1, b_2)$,
    \[
    F(b_1, b_2) - F(b_1, a_2) - F(a_1, b_2) + F(a_1, a_2) \geq 0. 
    \]
  \end{itemize}
\end{remark}

Now we can connect the notion of CDF with randomness.

Suppose $X$-random real-valued variable on $(\Omega, \Sigma, \Pr)$
that is almost everywhere finite. Define 
\[
F_X (t) = \Pr \left\{ X(\omega) \leq t \right\}
\]
for $- \infty < t < \infty$. It can be verified that 
$F_X$ is a CDF.

Conversely, for any CDF $F$, there exists a probability space 
$(\Omega, \Sigma, \Pr)$ and a real valued random variable 
on $(\Omega, \Sigma, \Pr)$ with CDF $F$.

\begin{defi}
  If $X$ is RV on $(\Omega, \Sigma, \Pr)$ real valued and a.e. finite.
  Then we can define the induced Borel probability measure
  $\mu_X$ on $(\R, \bcal_\R)$ by 
  \[
  \mu_X (B) := \Pr \left\{ X \in B \right\}
  \]
  for all $B \in \bcal_\R$.
\end{defi}

Now suppose $\mu$ is any Borel probability measure on $\R$. Consider 
probability space $(\R, \bcal_\R, \mu)$ and formal identity mapping 
$\id$ on $\R$. Then $\mu_{\id} \equiv \mu$.

\begin{thm}
  There is a one-to-one correspondence between the family of CDFs 
  and the family of Borel probability measure on $\R$.
\end{thm}

\begin{proof}
  For any Borel probability measure $\mu$, 
  $F_\mu (t) = \mu((-\infty, t])$
  is a valid CDF.

  Conversely, for any CDF $F$, there exists unique probability
  measure $\mu_F$ on $\R$ such that $\mu_F ((-\infty, t]) = F(t)$
  for all $-\infty < t < \infty$. This is a corollary of Caratheorody
  extension theorem. For detailed proof see notes or textbook.
\end{proof}

\begin{remark}
  Suppose $X = (X_1, X_2)$ is a random vector in $\R^2$. We can 
  define 
  \[
  F_X (s, t) = \Pr \left\{ X_1 \leq s, X_2 \leq t \right\}.
  \]
  Corresponding results are also true.
\end{remark}

\begin{defi}[Probability mass function]
  Let $(\Omega, \Sigma, \Pr)$ be a probability space and $X: \Omega 
  \to \R$ be RV. Suppose there exists $S \subset \R$ countable
  such that $\Pr \left\{ X \in S \right\} = 1$. We can define 
  the probability mass function (PMF) $f_X$ via 
  \[
  f_X (t) = \Pr \left\{ X = t \right\}
  \] 
  for $t \in \R$. Due to the restriction, this gives complete description 
  of the distribution, and we can construct CDF $F_X$ via 
  \[
  F_X (t) = \sum_{s \leq t} f_X (s).
  \]
  This sum makes sense since the $f_X (s) = 0$ for all but countably many 
  $s$. 
  Conversely, we can also reconstruct $f_X$ from a CDF $F_X$.
\end{defi}

\begin{defi}[Probability density function]
  Suppose $F$ is a CDF which is absolutely continuous. That is, 
  there exists Borel measurable non-negative function $\rho$ on $\R$ 
  such that 
  \[
  F(t) = \int_{-\infty}^t \rho (s) \d s
  \]
  for all $-\infty < t < \infty$. 
  This implies $F$ is almost everywhere differentiable and the derivative 
  is $\rho$. In this case, say $\rho$ is the density function.

  If RV $X$ is such that $F_X$ is absolutely continuous, then the 
  corresponding $\rho_X$ is the probability density function for $X$.
\end{defi}

\begin{remark}
  Recall that a Borel $\sigma$-finite measure $\mu$ on the real line 
  is absolutely continuous w.r.t the Lebesgue measure $m$ on $\R$ if 
  $\mu (A) = 0$ whenever $A \in \bcal_\R$ is Lebesgue null.
  In this case, Randon-Nikodym theorem implies existence of non-negative 
  Borel measurable function $f$ such that $\mu (A) = \int_A f \d m$.
\end{remark}

\begin{thm}
  Suppose $X$ RV on $(\Omega, \Sigma, \Pr)$ is real-valued and a.e. finite.
  The following are equivalent:
  \begin{enumerate}
    \item $F_X$ is absolutely continuous.
    \item $\mu_X$ is absolutely containing w.r.t. Lebesgue measure.
  \end{enumerate}
  Moreover, $\rho_X$ is also the derivative of $\mu_X$ w.r.t. Lebesgue 
  measure. That is, for any $A \in \bcal_\R$, 
  \[
  \mu_X (A) = \int_A \rho_X (t) \d t.
  \]
\end{thm}

\subsection{Independence}

\begin{defi}
  Say two events $A, B \in \Sigma$ are independent if 
  $\Pr (A \cap B) = \Pr (A) \Pr (B)$.
\end{defi}

It is easy to verify that $A, B$ are independent implies 
$A^c, B$ are independent.

\begin{remark}
  Suppose $\Pr (B) > 0$, then the conditional probability of $A$ 
  given $B$ is defined as 
  \[
  \Pr (A \mid B) = \frac{\Pr (A \cap B)}{\Pr (B)}.
  \]
  Then, independence of $A$ and $B$ is equivalent to 
  $\Pr (A \mid B) = \Pr (A)$.
\end{remark}

\begin{defi}
  Let $A_1, \dots, A_n$ be events. Say they are mutually independent
  if for any $\emptyset \neq I \subset [n]$, we have 
  \[
  \Pr \left( \bigcap_{i \in I} A_i \right) = \prod_{i \in I} \Pr (A_i).
  \]
  This is equivalent to saying that for every $2 \leq i \leq n$, 
  the event $A_i$ is independent from any event generated by 
  $A_1, \dots, A_{i - 1}$, or $A_i$ is independent from 
  $\sigma \left( A_1, \dots, A_{i - 1} \right)$.
\end{defi}

\begin{remark}
  The events $A_1, \dots, A_n$ are called $k$-wise independent if 
  any $k$-subset of the events are mutually independent. For $k < n$, 
  this notion is strictly weaker than mutual independence of all 
  $n$ events. As an example, consider $\Pr$ to be the uniform distribution
  on $\left\{ 1, \dots, 4 \right\}$. Let $A_1 = \left\{ 1, 2 \right\}$,
  $A_2 = \left\{ 1, 3 \right\}$, and $A_3 = \left\{ 2, 3 \right\}$.
  Then they are pairwise independent but not mutually independent.
\end{remark}

\begin{defi}
  A collection of events $\{A_\lambda\}_{\lambda \in \Lambda}$ 
  on $(\Omega, \Sigma, \Pr)$ are mutually independent if any finite subset 
  of events are mutually independent.
\end{defi}

\begin{defi}
  Let $(\Omega, \Sigma, \Pr)$ be a probability space. Two  
  $\sigma$-subfields are independent if for any $A \in \Sigma_1$ 
  and $B \in \Sigma_2$, $A, B$ are independent.
\end{defi}

\begin{defi}
  Let $(\Omega, \Sigma, \Pr)$ be a probability space and 
  $X, Y$ be two real-valued random variables. Say $X$ and $Y$ 
  are independent if 
  \[
  \Pr \left\{ X \in A, Y \in B \right\} = \Pr \left\{ X \in A \right\}
  \Pr \left\{ Y \in B \right\}
  \]
  for any $A, B \in \bcal_\R$. 
  
  Equivalently, let $\Sigma_X, \Sigma_Y$ 
  be the $\sigma$-field generated by $X$ and $Y$. Then independence of 
  $X$ and $Y$ is equivalent to independence of $\Sigma_X$ and $\Sigma_Y$.
\end{defi}

Now we explore how this connect with product structure.

\begin{prop}
  Let $(\Omega_1, \Sigma_1, \Pr_1)$ and $(\Omega_2, \Sigma_2, \Pr_2)$
  be two probability spaces and let $(\Omega, \Sigma, \Pr)$ be the 
  product space. Let $X$ and $Y$ be two random variables on
  $(\Omega, \Sigma, \Pr)$. Suppose there exists some measurable
  functions such that $X (\omega_1, \omega_2) = g(\omega_1)$,
  and $Y(\omega_1, \omega_2) = h (\omega_2)$. Then $X$ and $Y$ are 
  independent.
\end{prop}

\begin{proof}
  Let $A, B \in \bcal_\R$. Then 
  \begin{align*}
    \Pr \left\{ X \in A, Y \in B \right\} 
    &= \Pr \left\{ (\omega_1, \omega_2) : \omega_1 \in g^{-1} (A), 
    \omega_2 \in h^{-1} (B) \right\} \\
    &= \Pr \left( \left\{ \omega_1 \in g^{-1} (A) \right\} \times 
    \left\{ \omega_2 \in h^{-1} (B) \right\} \right) \\
    &= \Pr_1 \left( \omega_1 \in g^{-1} (A) \right)
    \Pr_2 \left( \omega_2 \in h^{-1} (B) \right).
  \end{align*}
  However, 
  \begin{align*}
    \Pr_1 \left( \omega_1 \in g^{-1} (A) \right) 
    &= \Pr \left\{ (\omega_1, \omega_2) : \omega_1 \in g^{-1} (A), 
    \omega_2 \in \Omega_2 \right\} \\
    &= \Pr \left\{ X \in A \right\},
  \end{align*}
  and similarly for $Y$.
\end{proof}

\begin{remark}
  Let $(\Omega, \Sigma, \Pr)$ be a probability space and suppose 
  $X, Y$ be two random variables that are independent and 
  a.e. finite. 
  They then generate two Borel probability measure $\mu_X$ and 
  $\mu_Y$ on $\R$.
  Define a product probability space as 
  of $(\R^2, \bcal_{\R^2}, \mu_X \otimes \mu_Y)$. Define 
  $\tilde{X} (x, y) = x$ and $\tilde{Y} (x, y) = y$ as random 
  variables on the product space. By definition, 
  $\tilde{X}$ is equidistributed with $X$. That is, $\mu_{\tilde{X}} = \mu_X$
  and $F_{\tilde{X}} = F_X$. Similarly $\mu_{\tilde{Y}} = \mu_Y$.
  Also, $\tilde{X}, \tilde{Y}$ are independent. Now 
  $(X, Y)$ and $(\tilde{X}, \tilde{Y})$ have the same distribution.
\end{remark}

\begin{remark}
  If $X$ and $Y$ are independent, then their joint distribution 
  $F_{(X, Y)}$ is uniquely determined by the individual distributions
  of $F_X, F_Y$. Indeed,
  \[
  F_{(X, Y)} (s, t) = F_X (s) F_Y (t).
  \]
\end{remark}

\begin{remark}
  Let $(\Omega, \Sigma, \Pr)$ be a probability space and suppose 
  $X, Y$ be two random variables that are independent. Suppose they have 
  densities $\rho_X, \rho_Y$, then the distribution density of 
  vector $(X, Y)$ is $\rho_{(X, Y)} (s, t) = \rho_X (s) \rho_Y (t)$.
\end{remark}

\end{document}