\documentclass[a4paper]{article}
\usepackage{parskip}
\usepackage{lipsum}
% \usepackage{newtxtext}
\usepackage{eucal}
\linespread{1.15}

\def\nterm {Spring}
\def\nyear {2026}
\def\ncourse {Probability}

\newcommand{\convd}{\Rightarrow}
\newcommand{\convw}{\xrightarrow{w}}
\newcommand{\convp}{\xrightarrow{p}}

\input{../header}
\author{Notes taken by \nauthor\vspace{5pt}\\
Lectures by Konstantin Tikhomirov\vspace{5pt}\\
Carnegie Mellon University}

\newcommand{\TODO}{\textcolor{red}{\textbf{*** TO-DO ***}}}

\begin{document}
\maketitle

\tableofcontents

\section{Measure theory review}

\subsection{Measurable space and mapping}

\begin{defi}[$\sigma$-field]
  A collection of subsets $\Sigma \subset 2^{\Omega}$ is a $\sigma$-field if
  \begin{itemize}
    \item $\emptyset \in \Sigma$.
    \item If $A \in \Sigma$, then $A^c \in \Sigma$.
    \item If $\seqinfi{A_i} \subset \Sigma$, then $\cupinfi A_i \in \Sigma$.
  \end{itemize}
  The pair $(\Omega, \Sigma)$ is called a measurable space.
\end{defi}

\begin{defi}[atom]
Let $\Sigma$ be a $\sigma$-field. Say $A \in \Sigma$ is an atom
if for all $B \in \Sigma$ either $A \subset B$ or $A \cap B = \emptyset$.
\end{defi}

\begin{prop}
For all $\omega \in \Omega$, there exists atom $A \in \Sigma$ containing
$\omega$ if $\Omega$ is finite or countable.
\end{prop}

\begin{proof}
  Define $\tilde{A} = \bigcap \left\{ B \in \Sigma :
  \omega \in B \right\}$. We can check that $\tilde{A} \in \Sigma$
  and $\tilde{A}$ is an atom containing $\omega$.

\end{proof}

\begin{cor}
If $\Omega$ is finite or countable, there exists a partition
$\Omega = \bigsqcup_i \Omega_i$,
where each $\Omega_i$ is an atom of $\Sigma$. With this partition,
$\Sigma$ is just the power set with respect to $\left\{ \Omega_i \right\}_i$.
\end{cor}

\begin{defi}
If $F \subset 2^\Omega$, then the $\sigma$-field generated by $F$ is the
smallest $\sigma$-field containing all elements of $F$.
Write this $\sigma$-field as $\sigma (F)$.
\end{defi}

\begin{eg}
Let $\Omega = \left\{ 1, 2, 3, 4, 5 \right\}$ and
$F = \left\{ \left\{ 2,3 \right\}, \left\{ 3,4 \right\} \right\}$.
Construct $\sigma$-field $\Sigma$ generated by $F$.
$\Sigma$ is all possible union of sets from the collection
$\left\{ \{2\}, \{3\}, \{4\}, \{1, 5\}\right\}$.

\end{eg}

\begin{defi}[measurable mapping]
Given two measurable spaces $(\Omega, \Sigma)$ and $(\tilde{\Omega},
\tilde{\Sigma})$. Then $f : \Omega \to \tilde{\Omega}$ is measurable
if $f^{-1} (B) \in \Sigma$ for all $B \in \tilde{\Sigma}$.
\end{defi}

\begin{defi}[Borel $\sigma$-field]
Let $(T, \tau)$ be a topological space. Then the Borel $\sigma$-field
$\B(T, \tau)$ is defined as the smallest $\sigma$-field containing
all open sets.
\end{defi}

\begin{defi}[product measurable space]
Given two measurable spaces $(\Omega, \Sigma)$ and
$(\tilde{\Omega}, \tilde{\Sigma})$. We can define the product
measurable space as follows: let the ground set be
$\Omega \times \tilde{\Omega}$, and let
$\Sigma \otimes \tilde{\Sigma}$ be the smallest $\sigma$-field
containing all rectangles $B \times \tilde{B}$ where
$B \in \Sigma$ and $\tilde{B} \in \tilde{\Sigma}$.

More generally,
let $\Lambda$ be an index set and $(\Omega_\lambda, \Sigma_\lambda)_{\lambda
\in \Lambda}$. Define the product $\sigma$-field $\bigotimes_{\lambda
\in \Lambda} \Sigma_\lambda$ be
the smallest $\sigma$-field containing all elements
in the form of $\prod_{\lambda \in \Lambda} B_\lambda$ where
$B_\lambda \in \Sigma_\lambda$ and $B_\lambda = \Omega_\lambda$ for all
but countably many indices.
\end{defi}

\begin{prop}
  Let $\left( \Omega_i, \Sigma_i \right)_{i=1}^n$ be measurable
  spaces and $\left( \prod_{i=1}^n \Omega_i, \bigotimes_{i=1}^n \Sigma_i
   \right)$ be the product space. Let $(\Omega, \Sigma)$ be the domain
  and $f = (f_1, \dots, f_n) : (\Omega, \Sigma) \to (\prod_{i=1}^n
  \Omega_i, \bigotimes_{i=1}^n \Sigma_i)$. Suppose $f$ is measurable,
  then every coordinate projection $f_i : \Omega \to \Omega_i$ is
  measurable.

  This is also true for arbitrary index set.
\end{prop}

\begin{prop}
  If $f : (\Omega, \Sigma) \to (\Omega_f, \Sigma_f)$ and
  $g : (\Omega, \Sigma) \to (\Omega_g, \Sigma_g)$, then the concatenation
  $(f, g)$ is measurable w.r.t. the product space
  $(\Omega_f \times \Omega_g, \Sigma_f \otimes \Sigma_g)$.
\end{prop}

\begin{proof}
  Let $A \times B$ be such that $A \in \Sigma_f$ and $B \in \Sigma_g$.
  Then the preimage
  \[
  (f, g)^{-1} (A \times B) = f^{-1} (A) \cap g^{-1} (B) \in \Sigma.
  \]
  By definition, the product $\sigma$-field is generated by rectangles,
  so the proof is complete.

\end{proof}

\subsection{Measure space}
\begin{defi}[measure]
  Let $(\Omega, \Sigma)$ be a measurable space. Then $\mu : \Sigma \to
  [0, \infty]$ is a measure if
  \begin{itemize}
    \item $\mu (\emptyset) = 0$.
    \item If $A_i \in \Sigma$ is pairwise disjoint then
    $\mu \left( \cupinfi A_i \right) = \suminfi \mu (A_i)$.
  \end{itemize}
\end{defi}

\begin{prop}[continuity of measure]
  If $A_1 \subset A_2 \subset \dots$ is a nested sequence of elements
  of $\Sigma$ and $\mu$ be any measure on $(\Omega, \Sigma)$. Then
  \[
  \mu \left( \cupinfi A_i \right) = \lim_{i \to \infty} \mu (A_i).
  \]

  If $A_1 \supset A_2 \supset \dots$ is a nested sequence of elements
  of $\Sigma$ and $\mu (A_n) < \infty$ for some $n$. Then
  \[
  \mu \left( \capinfi A_i \right) = \lim_{i \to \infty} \mu (A_i).
  \]
\end{prop}

\begin{defi}
  Let $(\Omega, \Sigma, \mu)$ be a measure space.

  Say $\mu$ is $\sigma$-finite
  if there is a representation $\Omega = \cupinfi \Omega_i$ where
  $\Omega_i \in \Sigma$ and $\mu (\Omega_i) < \infty$.

  Say $\mu$ is a probability measure if $\mu(\Omega) = 1$.
\end{defi}

\begin{defi}[completion of measure space]
  Let $(\Omega, \Sigma, \mu)$ be a measure space. Let
  \[
  \tilde{\Sigma} = \left\{ A \cup B : A \in \Sigma,
  B \subset \Omega, \text{there exists $C \in \Sigma$ with
  $\mu(C) = 0$ and $B \subset C$} \right\}.
  \]
  We can check $\tilde{\Sigma}$ is a $\sigma$-field. If $\tilde{\mu}$
  is a measure on $(\Omega, \tilde{\Sigma})$ which agrees with $\mu$
  on $\Sigma$, then $(\Omega, \tilde{\Sigma}, \tilde{\mu})$ is called
  a completion of $(\Omega, \Sigma, \mu)$.
\end{defi}

\subsection{$\pi$-$\lambda$ theorem}

\begin{defi}[$\pi$-system]
  Let $\Omega$ be a set and $\P$ be a collection of subsets of $\Omega$.
  Then $\P$ is a $\pi$-system if it is closed with respect to
  taking finite intersections. That is, $A, B \in \P$ implies $A \cap B
  \in \P$.
\end{defi}

\begin{eg}
  On the real line $\R$, both $\P_1 = \left\{ (a, b) : a < b \right\}$
  and $\P_2 = \left\{ (-\infty, a] : a \in \R \right\}$ are
  $\pi$-systems.

\end{eg}

\begin{defi}[$\lambda$-system]
  Let $\Omega$ be a set and $\L$ be a collection of subsets of $\Omega$.
  Say $\L$ is a $\lambda$-system if
  \begin{itemize}
    \item $\emptyset \in \L$.
    \item $A \in \L$ implies $A^c \in \L$.
    \item for all countable collection of disjoint elements $A_i \in \L$,
    we have $\cupinfi A_i \in \L$.
  \end{itemize}

  For an alternative definition, say $\L$ is a $\lambda$-system if 
  \begin{itemize}
    \item $\Omega \in \L$.
    \item If $A, B \in \L$ and $A \subset B$, then $B \setminus A \in \L$.
    \item If $A_n \in \L$ and $A_n \uparrow A$, then $A \in \L$.
  \end{itemize}
\end{defi}

\begin{thm}[$\pi$-$\lambda$ theorem]
  Let $\Omega$ be a set, $\P$ be a $\pi$-system and $\L$ be a $\lambda$-system.
  Also suppose $\P \subset \L$, then $\sigma (\P) \subset \L$.
\end{thm}

\begin{proof}
  Let $\ell (\P)$ be the smallest $\lambda$-system on $\Omega$ containing
  $\P$. The goal is to show that $\ell (\P)$ is a $\sigma$-field.
  We need to show that if $A_i \in \ell (\P)$ for $1 \leq i < \infty$,
  then $\cupinfi A_i \in \ell (\P)$. Note that
  \[
  \cupinfi A_i = \cupinfi \left( A_i \setminus \cupj^{i - 1} A_i \right),
  \]
  so it suffices to show that $A, B \in \ell (\P)$ implies $A \cap B \in
  \ell (\P)$.

  For $A \in \ell(\P)$ we define
  \[
  W_A = \left\{ B \subset \Omega : A \cap B \in \ell (\P) \right\}.
  \]
  It can be directly verified that $W_A$ is a $\lambda$-system.

  Take $A \in \P$, then for any $B \in \P$ we have $A \cap B \in \P
  \subset \ell (\P)$. Hence, $\P \subset W_A$ and thus $\ell (\P) \subset W_A$
  for all $A \in \P$, as $\ell (\P)$ is the smallest $\lambda$-system on
  $\Omega$ containing $\P$.
  Now take $A \in \ell (\P)$, we have $A \in W_B$
  for all $B \in \P$. It follows that $A \cap B \in \ell (\P)$ and thus
  $B \in W_A$. Hence similarly
  $\ell (\P) \subset W_A$ for all $A \in \ell (\P)$.

  Now for any pair $B, C \in \ell (\P)$, we have $C \in W_B$ and thus
  $B \cap C \in \ell (\P)$. This completes the proof.

\end{proof}

\subsection{Extension theorems}

\begin{defi}[semi-field]
  A collection of subsets $S \subset 2^{\Omega}$ is a semi-field if
  \begin{itemize}
    \item $\emptyset \in S$ and $\Omega \in S$.
    \item $A, B \in S$ implies $A \cap B \in S$.
    \item If $A \in S$, then $A^c$ is a finite disjoint union of sets in $S$.
  \end{itemize}
\end{defi}

\begin{thm}[Caratheorody's extension theorem]
  Let $S$ be a semi-field and let $\mu$ be a non-negative function on $S$
  satisfying:
  \begin{itemize}
    \item $\mu (\emptyset) = 0$.
    \item If $A_1, \dots, A_n$ are disjoint and $\cupi^n A_i \in S$, then
    $\mu (\cupi^n A_i) = \sumi^n \mu (A_i)$.
    \item If $A_1, A_2, \dots$ are such that $\cupinfi A_i \in S$, then
    $\mu (\cupinfi A_i) \leq \suminfi \mu (A_i)$.
  \end{itemize}
  Then $\mu$ admits a unique extension $\bar{\mu}$ which is a
  measure on $\bar{S}$, the field (algebra) generated by $S$. Moreover,
  if $\bar{\mu}$ is $\sigma$-finite then $\bar{\mu}$ admits a unique
  extension $\tilde{\mu}$ to $\sigma (S)$.
\end{thm}

\begin{notation}
  Let $T$ be any set. Write
  \[
  \R^T = \left\{ \left( \omega_t \right)_{t \in T} :
  \omega_t \in \R \right\}.
  \]
  Also write $\rcal^T$ as the $\sigma$-field generated by rectangles
  of the form $\prod_{t \in T} I_t$, where for each $t \in T$,
  $I_t$ is either a semi-open interval of the form $(a, b]$
  with $a < b$ or $I_t = \R$, and $I_t = \R$ for all but finitely
  many $t \in T$.
\end{notation}

\begin{thm}[Kolmogorov's extension theorem]
  For each finite non-empty subset $J \subset T$,
  let $\mu_J$ be a Borel probability measure in $\R^J$,
  and assume that the measures $\left( \mu_J \right)_{J \subset T,
  \abs{J} < \infty}$ are
  compatible, in the sense that whenever $J_1 \subset J_2 \subset T$
  with $0 \leq \abs{J_1} \leq \abs{J_2} < \infty$,
  $I_j \subset \R$ with $j \in J_1$ are Borel subsets of $\R$, and
  \[
  \tilde{I_j} = \begin{cases}
    I_j & (j \in J_1) \\
    \R & (j \in J_2 \setminus J_1),
  \end{cases}
  \]
  one has
  \[
  \mu_{J_2} \left( \prod_{j \in J_2} \tilde{I_{j}} \right)
  = \mu_{J_1} \left( \prod_{j \in J_1} I_j \right).
  \]
  Then there exists a unique probability measure $\mu$ on
  $(\R^T, \rcal^T)$ consistent with $\left( \mu_J \right)_{J \subset T,
  \abs{J} < \infty}$. That is, one has
  \[
  \mu \left( \prod_{t \in T} I_t \right)
  = \mu_J \left( \prod_{j \in J} I_j \right)
  \]
  whenever $J \subset T$ with $\abs{J} < \infty$ and $I_t = \R$
  for all $t \notin J$.
\end{thm}

\subsection{Lebesgue Integration}

Here we provide a proof for dominated convergence theorem that uses
the truncation technique, which will be a useful technique later
in the course.

\begin{thm}[dominated convergence theorem]
  Let $\seqinfn{f_n}$ be a sequence of measurable
  functions on $(\Omega, \Sigma, \mu)$ and $g \geq 0$ be another measurable
  function. Suppose
  \begin{enumerate}
    \item $\int g \d \mu < \infty$.
    \item $\abs{f_n} (\omega) \leq g (\omega)$ for all $\omega \in \Omega$
    and $n \geq 1$.
    \item $f_n \to f$ pointwise.
  \end{enumerate}
  Then
  \[
  \lim_{n \to \infty} \int f_n \d \mu = \int f \d \mu.
  \]
\end{thm}

\begin{proof}
  \textbf{Claim 1.}
  If $h$ is a function on $(\Omega, \Sigma, \mu)$ with
  $h \geq 0$ and $\int h \d \mu < \infty$. Let $\seqinfn{A_n}$
  be any sequence of elements of $\Sigma$ with $\mu (A_n) \to 0$.
  Then
  \[
  \int_{A_n} h \d \mu \to 0.
  \]
  Proof of claim. WLOG assume $\mu(A_n) \leq 2^{-n}$ for all $n$.
  Define $h_n = h \ind_{\bigcup_{i=n}^\infty A_i}$. We then have
  \begin{enumerate}
    \item The sequence $\seqinfn{h_n}$ is monotone.
    \item $h_n$ converges to $0$ almost everywhere.
  \end{enumerate}
  Monotone convergence theorem then implies $\lim_{n \to \infty}
  \int h_n \d \mu = 0$. Meanwhile,
  \[
  0 \leq \int_{A_n} h \d \mu \leq \int h_n \d \mu,
  \]
  and the proof is complete.

  \textbf{Claim 2.}
  Suppose $h \geq 0$ and $\int h \d \mu < \infty$.
  Let $\seqinfn{\epsilon_n}$ be a sequence of strictly positive numbers
  converging to zero. Define
  \[
  B_n = \left\{ \omega \in \Omega : h (\omega) \leq \epsilon_n \right\}
  \in \Sigma.
  \]
  Then
  \[
  \int_{B_n} h \d \mu \to 0.
  \]
  Proof of this claim is left as an exercise.

  Now we prove the theorem. Fix $\epsilon > 0$. By the previous two
  claims, there exists $M > 0$ and $\delta > 0$ such that
  \[
  \int_{\left\{ g \geq M \right\}} g \d \mu < \epsilon,
  \quad
  \int_{\left\{ g \leq \delta \right\}} g \d \mu < \epsilon.
  \]
  Let $U = \left\{ \omega : \delta < g(\omega) < M \right\}$.
  Since $g$ is integrable, $\mu (U) < \infty$.
  For $\omega \in U$, let $n_\epsilon (\omega)$ be the smallest
  index such that $n \geq n_\epsilon (\omega)$ implies
  $\abs{f_n (\omega) - f(\omega)} \leq \epsilon \mu(U)^{-1}$.
  It follows that there exists $N$ such that
  \[
  \mu \left( \left\{ \omega \in U: n_\epsilon (\omega) > N \right\}  \right)
  \leq \frac{\epsilon}{M}.
  \]
  Then, for $n \geq N$, we have
  \begin{align*}
    \abs{\int_U (f_n - f) \d \mu}
    \leq \int_{n_\epsilon (\omega) \leq N} \abs{f_n - f} \d \mu
    + \int_{n_\epsilon (\omega) > N} \abs{f_n - f} \d \mu
    \leq 3 \epsilon.
  \end{align*}
  Now for $n \geq N$, we have
  \[
  \abs{\int (f_n - f) \d \mu}
  \leq 3 \epsilon + \int_{U^c} \abs{f - f_n} \d \mu
  \leq 3 \epsilon + 2 \int_{U^c} g \d \mu \leq 7 \epsilon.
  \]

\end{proof}

\begin{thm}[Markov-Chebyshev inequality]
  Suppose we have probability measure space $(\Omega, \Sigma, \Pr)$
  and $f \geq 0$. Suppose also $\int f \d \Pr < \infty$. Then
  \[
  \Pr \left( \left\{ \omega : f(\omega) > t \right\} \right)
  \leq \frac{1}{t} \int f \d \Pr.
  \]
  for all $t > 0$.
\end{thm}

\begin{remark}
  Let $1 \leq p < \infty$. Suppose $f : (\Omega, \Sigma, \Pr) \to
  [0, \infty]$ and $\int f^p \d \Pr < \infty$. Then
  \[
  \Pr \left( \left\{ \omega : f (\omega) > t \right\} \right)
  \leq \frac{1}{t^p} \int f^p \d \Pr.
  \]
  for all $t > 0$.
\end{remark}

\begin{remark}
  Suppose $\int e^{\lambda f} \d \Pr < \infty$ for all
  $\lambda \in \R$ and $f : (\Omega, \Sigma, \Pr) \to [0, \infty]$.
  Then
  \[
  \Pr \left( \left\{ \omega : f (\omega) > t \right\} \right)
  \leq \frac{1}{e^{\lambda t}} \int e^{\lambda f} \d \Pr
  \]
  for all $t > 0$ and $\lambda > 0$.
\end{remark}

\begin{thm}[H\"older inequality]
  Let $p, q \in [1, \infty]$ and $p^{-1} + q^{-1} = 1$.
  Let $(\Omega, \Sigma, \mu)$ be a probability space. For any
  measurable functions $f, g$, we have
  \[
  \int \abs{f g} \d \Pr \leq \left( \int \abs{f}^p \d \Pr \right)^{1 / p}
  \left( \int \abs{g}^q \d \Pr \right)^{1 / q}.
  \]
\end{thm}

\begin{thm}[Jensen's inequality]
  Let $(\Omega, \Sigma, \Pr)$ be a probability space and $f$ be
  integrable. Let $\phi : \bar{\R} \to \bar{\R}$ be convex
  and suppose $\phi (\infty) = \lim_{x \to \infty} \phi(x)$
  and $\phi (-\infty) = \lim_{x \to -\infty} \phi(x)$. Then
  \[
  \phi \left( \int f \d \Pr \right) \leq \int \phi (f) \d \Pr.
  \]
\end{thm}

\subsection{Product measures and Fubini theorem}
Let $(\Omega_1, \Sigma_1, \mu_1)$, $(\Omega_2, \Sigma_2, \mu_2)$
be $\sigma$-finite measure spaces. We already defined the product
$\Sigma_1 \otimes \Sigma_2$. To define a product measure, we first
consider the algebra of rectangles
\[
S = \left\{ A \in \Sigma_1 \otimes \Sigma_2 : A = A_1 \times A_2
\text{ for some $A_1 \in \Sigma_1, A_2 \in \Sigma_2$} \right\}.
\]
Then we can define $\mu = \mu_1 \times \mu_2$ on $S$ by
\[
\mu (A) = \mu_1 (A_1) \mu_2 (A_2)
\]
for $A = A_1 \times A_2$. We can check that the definition is
self-consistent. That is, if $A = A_1 \times A_2$ is a countable
union of disjoint rectangles
$\seqinfj{A_1^{(j)} \times A_2^{(j)}}$, we have
\[
\mu (A_1 \times A_2) = \suminfj \mu ( A_1^{(j)} \times A_2^{(j)} ).
\]
This can be verified with monotone convergence theorem.
Now $\mu$ is a premeasure and can be uniquely
extended to $\Sigma_1 \otimes \Sigma_2$.

\begin{thm}[Fubini-Tonelli]
  Let $(\Omega_1, \Sigma_1, \mu_1)$, $(\Omega_2, \Sigma_2, \mu_2)$
  be $\sigma$-finite measure spaces and let $(\Omega, \Sigma, \mu)$
  be the product space. Suppose $f$ is measurable on the product space.
  Suppose either $f$ is non-negative or $\int_\Omega \abs{f} \d \mu <
  \infty$. Then
  \begin{itemize}
    \item $y \mapsto f(x, y)$ is $\Sigma_2$ measurable for all $x \in \Omega_1$.
    \item $x \mapsto \int_{\Omega_2} f(x, y) \d \mu_2 (y)$ is
    $\Sigma_1$ measurable.
    \item We have
    \[
    \int_{\Omega_1} \int_{\Omega_2} f(x, y) \d \mu_2 (y) \d \mu_1 (x)
    = \int_\Omega f(x, y) \d \mu(x, y).
    \]
  \end{itemize}
\end{thm}

\begin{proof}
  First suppose $f = \ind_{A}$ for $A \in \Sigma$.
  Also suppose $\mu_1, \mu_2$ are finite. Define section
  \[
  A_x = \left\{ y \in \Omega_2 : (x, y) \in A \right\}.
  \]
  The goal is to show that $A_x \in \Sigma_2$ for all $x \in \Omega_1$.
  Define a family of sets
  \[
  \fcal_x = \left\{ B \in \Sigma : \text{$B_x$ is $\Sigma_2$-measurable} \right\}.
  \]
  It can be verified that $\fcal_x$ is a $\sigma$-field for all $x \in \Omega_1$.
  Also, $\fcal_x$ contains all rectangles and thus $\Sigma \subset \fcal_x$.
  Hence, we have shown that $y \mapsto \ind_A (x, y) = \ind_{A_x} (y)$
  is measurable for all $x \in \Omega_1$.

  Next we show $x \mapsto \mu_2 (A_x)$ is measurable
  and its integral over $\Omega_1$ is equal to $\mu (A)$.
  Define
  \[
  \ucal = \left\{ B \in \Sigma : \text{$x \mapsto \mu_2(B_x)$ is
  $\Sigma_1$-measurable and $\int_{\Omega_1} \mu_2(B_x) \d \mu_1 = \mu (B)$} \right\}
  \]
  It can be verified that $\ucal$ is a $\lambda$-system.
  Note that $\ucal$ also contains all rectangles in $\Sigma$.
  It follows that $\ucal = \Sigma$ and the proof for
  indicator functions are complete.

  Then use linearity to extend to simple functions, and use monotone
  convergence theorem to prove the statement for non-negative functions.
  For the case where $f$ is integrable, consider the positive and
  negative part about $f$ to complete the proof.

\end{proof}

\section{Probability theory basics}

\subsection{Distributions and densities}

\begin{defi}
  Let $F : \R \to [0, 1]$. Suppose $F$ is
  \begin{itemize}
    \item right-continuous.
    \item non-decreasing.
    \item $\lim_{t \to -\infty} F(t) = 0$ and
    $\lim_{t \to \infty} f(t) = 1$.
  \end{itemize}
  Then $F$ is a cumulative distribution function (CDF).
\end{defi}

\begin{remark}
  If we want to define CDF in $\R^2$ then the axioms are
  \begin{itemize}
    \item right-continuous: $F(\tilde{s}, \tilde{t})
    \to F(s, t)$ as $t \downarrow \tilde{t}$ and
    $s \downarrow \tilde{s}$.
    \item coordinate-wise non-decreasing.
    \item $\lim_{s, t \to \infty} F(s, t) = 1$,
    $\lim_{s \to -\infty} F(s, t) = 0$ for any $t$, and
    $\lim_{t \to - \infty} F (s, t) = 0$ for any $s$.
    \item For a rectangle with bottom left vertex $(a_1, a_2)$
    and top right vertex $(b_1, b_2)$,
    \[
    F(b_1, b_2) - F(b_1, a_2) - F(a_1, b_2) + F(a_1, a_2) \geq 0.
    \]
  \end{itemize}
\end{remark}

Now we can connect the notion of CDF with randomness.

Suppose $X$-random real-valued variable on $(\Omega, \Sigma, \Pr)$
that is almost everywhere finite. Define
\[
F_X (t) = \Pr \left( X(\omega) \leq t \right)
\]
for $- \infty < t < \infty$. It can be verified that
$F_X$ is a CDF.

Conversely, for any CDF $F$, there exists a probability space
$(\Omega, \Sigma, \Pr)$ and a real valued random variable
on $(\Omega, \Sigma, \Pr)$ with CDF $F$.

\begin{defi}
  If $X$ is random variable on $(\Omega, \Sigma, \Pr)$ real valued and a.e.\ finite.
  Then we can define the induced Borel probability measure
  $\mu_X$ on $(\R, \bcal_\R)$ by
  \[
  \mu_X (B) := \Pr \left( X \in B \right)
  \]
  for all $B \in \bcal_\R$.
\end{defi}

Now suppose $\mu$ is any Borel probability measure on $\R$. Consider
probability space $(\R, \bcal_\R, \mu)$ and formal identity mapping
$\id$ on $\R$. Then $\mu_{\id} \equiv \mu$.

\begin{thm}
  There is a one-to-one correspondence between the family of CDFs
  and the family of Borel probability measure on $\R$.
\end{thm}

\begin{proof}
  For any Borel probability measure $\mu$,
  $F_\mu (t) = \mu((-\infty, t])$
  is a valid CDF.

  Conversely, for any CDF $F$, there exists unique probability
  measure $\mu_F$ on $\R$ such that $\mu_F ((-\infty, t]) = F(t)$
  for all $-\infty < t < \infty$. This is a corollary of Caratheorody
  extension theorem. For detailed proof see notes or textbook.

\end{proof}

\begin{remark}
  Suppose $X = (X_1, X_2)$ is a random vector in $\R^2$. We can
  define
  \[
  F_X (s, t) = \Pr \left( X_1 \leq s, X_2 \leq t \right).
  \]
  Corresponding results are also true.
\end{remark}

\begin{defi}[Probability mass function]
  Let $(\Omega, \Sigma, \Pr)$ be a probability space and $X: \Omega
  \to \R$ be random variable. Suppose there exists $S \subset \R$ countable
  such that $\Pr \left( X \in S \right) = 1$. We can define
  the probability mass function (PMF) $f_X$ via
  \[
  f_X (t) = \Pr \left( X = t \right)
  \]
  for $t \in \R$. Due to the restriction, this gives complete description
  of the distribution, and we can construct CDF $F_X$ via
  \[
  F_X (t) = \sum_{s \leq t} f_X (s).
  \]
  This sum makes sense since the $f_X (s) = 0$ for all but countably many
  $s$.
  Conversely, we can also reconstruct $f_X$ from a CDF $F_X$.
\end{defi}

\begin{defi}[Probability density function]
  Suppose $F$ is a CDF which is absolutely continuous. That is,
  there exists Borel measurable non-negative function $\rho$ on $\R$
  such that
  \[
  F(t) = \int_{-\infty}^t \rho (s) \d s
  \]
  for all $-\infty < t < \infty$.
  This implies $F$ is almost everywhere differentiable and the derivative
  is $\rho$. In this case, say $\rho$ is the density function.

  If random variable $X$ is such that $F_X$ is absolutely continuous, then the
  corresponding $\rho_X$ is the probability density function for $X$.
\end{defi}

\begin{remark}
  Recall that a Borel $\sigma$-finite measure $\mu$ on the real line
  is absolutely continuous w.r.t the Lebesgue measure $m$ on $\R$ if
  $\mu (A) = 0$ whenever $A \in \bcal_\R$ is Lebesgue null.
  In this case, Randon-Nikodym theorem implies existence of non-negative
  Borel measurable function $f$ such that $\mu (A) = \int_A f \d m$.
\end{remark}

\begin{thm}
  Suppose $X$ RV on $(\Omega, \Sigma, \Pr)$ is real-valued and a.e.\ finite.
  The following are equivalent:
  \begin{enumerate}
    \item $F_X$ is absolutely continuous.
    \item $\mu_X$ is absolutely continuous w.r.t. Lebesgue measure.
  \end{enumerate}
  Moreover, $\rho_X$ is also the derivative of $\mu_X$ w.r.t. Lebesgue
  measure. That is, for any $A \in \bcal_\R$,
  \[
  \mu_X (A) = \int_A \rho_X (t) \d t.
  \]
\end{thm}

\subsection{Independence}

\begin{defi}
  Say two events $A, B \in \Sigma$ are independent if
  $\Pr (A \cap B) = \Pr (A) \Pr (B)$.
\end{defi}

It is easy to verify that $A, B$ are independent implies
$A^c, B$ are independent.

\begin{remark}
  Suppose $\Pr (B) > 0$, then the conditional probability of $A$
  given $B$ is defined as
  \[
  \Pr (A \mid B) = \frac{\Pr (A \cap B)}{\Pr (B)}.
  \]
  Then, independence of $A$ and $B$ is equivalent to
  $\Pr (A \mid B) = \Pr (A)$.
\end{remark}

\begin{defi}
  Let $A_1, \dots, A_n$ be events. Say they are mutually independent
  if for any $\emptyset \neq I \subset [n]$, we have
  \[
  \Pr \left( \bigcap_{i \in I} A_i \right) = \prod_{i \in I} \Pr (A_i).
  \]
  This is equivalent to saying that for every $2 \leq i \leq n$,
  the event $A_i$ is independent from any event generated by
  $A_1, \dots, A_{i - 1}$, or $A_i$ is independent from
  $\sigma \left( A_1, \dots, A_{i - 1} \right)$.
\end{defi}

\begin{remark}
  The events $A_1, \dots, A_n$ are called $k$-wise independent if
  any $k$-subset of the events are mutually independent. For $k < n$,
  this notion is strictly weaker than mutual independence of all
  $n$ events. As an example, consider $\Pr$ to be the uniform distribution
  on $\left\{ 1, \dots, 4 \right\}$. Let $A_1 = \left\{ 1, 2 \right\}$,
  $A_2 = \left\{ 1, 3 \right\}$, and $A_3 = \left\{ 2, 3 \right\}$.
  Then they are pairwise independent but not mutually independent.
\end{remark}

\begin{defi}
  A collection of events $\{A_\lambda\}_{\lambda \in \Lambda}$
  on $(\Omega, \Sigma, \Pr)$ are mutually independent if any finite subset
  of events are mutually independent.
\end{defi}

\begin{defi}
  Let $(\Omega, \Sigma, \Pr)$ be a probability space. Two
  $\sigma$-subfields are independent if for any $A \in \Sigma_1$
  and $B \in \Sigma_2$, $A, B$ are independent.
\end{defi}

\begin{defi}
  Let $(\Omega, \Sigma, \Pr)$ be a probability space and
  $X, Y$ be two real-valued random variables. Say $X$ and $Y$
  are independent if
  \[
  \Pr \left( X \in A, Y \in B \right) = \Pr \left( X \in A \right)
  \Pr \left( Y \in B \right)
  \]
  for any $A, B \in \bcal_\R$.

  Equivalently, let $\Sigma_X, \Sigma_Y$
  be the $\sigma$-field generated by $X$ and $Y$. Then independence of
  $X$ and $Y$ is equivalent to independence of $\Sigma_X$ and $\Sigma_Y$.
\end{defi}

Now we explore how this connect with product structure.

\begin{prop}
  Let $(\Omega_1, \Sigma_1, \Pr_1)$ and $(\Omega_2, \Sigma_2, \Pr_2)$
  be two probability spaces and let $(\Omega, \Sigma, \Pr)$ be the
  product space. Let $X$ and $Y$ be two random variables on
  $(\Omega, \Sigma, \Pr)$. Suppose there exists some measurable
  functions such that $X (\omega_1, \omega_2) = g(\omega_1)$,
  and $Y(\omega_1, \omega_2) = h (\omega_2)$. Then $X$ and $Y$ are
  independent.
\end{prop}

\begin{proof}
  Let $A, B \in \bcal_\R$. Then
  \begin{align*}
    \Pr \left( X \in A, Y \in B \right)
    &= \Pr \left( (\omega_1, \omega_2) : \omega_1 \in g^{-1} (A),
    \omega_2 \in h^{-1} (B) \right) \\
    &= \Pr \left( \left\{ \omega_1 \in g^{-1} (A) \right\} \times
    \left\{ \omega_2 \in h^{-1} (B) \right\} \right) \\
    &= \Pr_1 \left( \omega_1 \in g^{-1} (A) \right)
    \Pr_2 \left( \omega_2 \in h^{-1} (B) \right).
  \end{align*}
  However,
  \begin{align*}
    \Pr_1 \left( \omega_1 \in g^{-1} (A) \right)
    &= \Pr \left( (\omega_1, \omega_2) : \omega_1 \in g^{-1} (A),
    \omega_2 \in \Omega_2 \right) \\
    &= \Pr \left( X \in A \right),
  \end{align*}
  and similarly for $Y$.

\end{proof}

\begin{remark}
  Let $(\Omega, \Sigma, \Pr)$ be a probability space and suppose
  $X, Y$ be two random variables that are independent and
  a.e.\ finite.
  They then generate two Borel probability measure $\mu_X$ and
  $\mu_Y$ on $\R$.
  Define a product probability space as
  of $(\R^2, \bcal_{\R^2}, \mu_X \times \mu_Y)$. Define
  $\tilde{X} (x, y) = x$ and $\tilde{Y} (x, y) = y$ as random
  variables on the product space. By definition,
  $\tilde{X}$ is equidistributed with $X$. That is, $\mu_{\tilde{X}} = \mu_X$
  and $F_{\tilde{X}} = F_X$. Similarly $\mu_{\tilde{Y}} = \mu_Y$.
  Also, $\tilde{X}, \tilde{Y}$ are independent. Now
  $(X, Y)$ and $(\tilde{X}, \tilde{Y})$ have the same distribution.
\end{remark}

\begin{remark}
  If $X$ and $Y$ are independent, then their joint distribution
  $F_{(X, Y)}$ is uniquely determined by the individual distributions
  of $F_X, F_Y$. Indeed,
  \[
  F_{(X, Y)} (s, t) = F_X (s) F_Y (t).
  \]
\end{remark}

\begin{remark}
  Let $(\Omega, \Sigma, \Pr)$ be a probability space and suppose
  $X, Y$ be two random variables that are independent. Suppose they have
  densities $\rho_X, \rho_Y$, then the distribution density of
  vector $(X, Y)$ is $\rho_{(X, Y)} (s, t) = \rho_X (s) \rho_Y (t)$.
\end{remark}

\begin{remark}
  If $X$ and $Y$ are independent random variable, and $f, g$ are measurable functions.
  Then $f(X)$ and $g(Y)$ are independent as well.
\end{remark}

\begin{remark}
  Given probability space $(\Omega, \Sigma, \Pr)$ and random variable $X$.
  It may not exists another random variable $Y$ that is independent from $X$
  on the same probability space.
  See the following example.
\end{remark}

\begin{eg}
  As an example, consider $([0, 1], \bcal_{[0, 1]}, m)$ and
  $X(\omega) = \omega$, so $X$ is uniform on $[0, 1]$. The goal is
  to construct variable $Y$ such that $Y \sim \Bernoulli (\frac{1}{2})$.

\end{eg}

\begin{proof}
  Let $A \in \bcal_{[0, 1]}$ and $t \in [0, 1]$. Define the density
  of set $A$ at point $t$ to be
  \[
  \lim_{\epsilon \to 0} \frac{m (A \cap [t - \epsilon, t + \epsilon])}
  {2 \epsilon}.
  \]
  The Lebesgue density theorem says this is well defined and
  takes values in $\left\{ 0, 1 \right\}$ for $m$-a.e.\ $t \in [0, 1]$.

  Suppose such $Y$ exists, we can vies $Y$ as an indicator of a set
  $A \subset [0, 1]$ of probability $\frac{1}{2}$. That is,
  $Y = \ind_A$ and $\Pr (A) = \frac{1}{2}$. Choose any point $t \in (0, 1)$
  such that density of $A$ at $t$ is well-defined. WLOG assume
  the density is $1$. Pick $\epsilon > 0$ such that
  $m (A \cap [t - \epsilon, t + \epsilon]) \geq \frac{3}{2} \epsilon$.
  Now,
  \begin{align*}
    m (A \cap [t - \epsilon, t + \epsilon])
    &= \Pr \left( Y = 1, X \in [t - \epsilon, t + \epsilon] \right) \\
    &= \Pr \left( Y = 1 \right) \Pr \left( X \in [t - \epsilon, t + \epsilon]
    \right) \\
    &= \epsilon,
  \end{align*}
  a contradiction.

  Alternatively, we can derive a contradiction using
  $m(A) = \Pr \left( Y = 1, X \in A \right)$.

\end{proof}

\begin{remark}
  The goal is to have statements ``independent'' from the underlying
  probability space.
\end{remark}

\subsection{Convolution}

\begin{defi}[convolution]
  Let $\mu$, $\nu$ be Borel probability measures on $\R$.
  The convolution of $\mu$ and $\nu$ is a probability measure
  on $\R$ such that
  \[
  (\mu * \nu) (S) = \int_{\R^2} \ind_S (x + y) d (\mu \times \nu)
  \]
  for all $S \in \bcal_{\R}$.
\end{defi}

\begin{remark}
  Suppose both $\mu$ and $\nu$ have densities $\rho_\mu$ and
  $\rho_\nu$. That is,
  $\mu \ll m$ and $\nu \ll m$. It follows that
  \begin{align*}
    (\mu * \nu) (S)
    &= \int_\R \left( \int_\R \ind_S (x + y) \rho_\mu (x) \d x \right)
    \rho_\nu (y) \d y \\
    &= \int_\R \left( \int_S \rho_\mu (w - y) \d w \right) \rho_\nu (y) \d y \\
    &= \int_S \left( \int_\R \rho_\mu (w - y) \rho_\nu (y) \d y \right)
    \d w
  \end{align*}
  Note that this implies $\mu * \nu \ll m$ and
  \[
  \rho_{\mu * \nu} (w) = \int_\R \rho_\mu (w - y) \rho_\nu (y) \d y,
  \]
  which is the convolution of funtion $\rho_\mu$ and $\rho_\nu$.
\end{remark}

\begin{remark}
  If $X$ and $Y$ are two independent random variables and
  $\mu_X$ and $\mu_Y$ are the corresponding induced Borel measure $\R$,
  then $\mu_{X + Y} = \mu_X * \mu_Y$.
\end{remark}

\begin{remark}
  Suppose $X$ and $Y$ are independent and their CDF is $F_X$ and $F_Y$,
  then 
  \[
  F_{X + Y} (t) = \int_\R F_X (t - w) \d \mu_Y (w)
  = \int_\R F_X (t - w) \d F_Y (w).
  \]
\end{remark}

\subsection{Moments}

In this section we explore the computation and basic properties of 
moments.

\begin{defi}[moments]
  Let $X$ be a random variable on $(\Omega, \Sigma, \Pr)$. 
  The $p$-th absolute moment of $X$ is 
  \[
  \E \abs{X}^p = \int_\Omega \abs{X}^p \d \Pr.
  \]
  This is always well-defined but can be infinite.

  If $p$ is positive integer, the $p$-th moment of $X$ is 
  \[
  \E X^p = \int_\Omega X^p \d \Pr
  \]
  whenever it is defined.

  In particular, $\E X$ is the mean or expectation, the variance is 
  $\var (X) = \E (X - \E X)^2$ whenever the expectation is defined,
  and the standard deviation is $\sqrt{\var (X)}$.
\end{defi}

\begin{prop}
  Let $X$ be random variable $(\Omega, \Sigma, \Pr)$ and 
  $0 < p < q < \infty$. Then, 
  \[
  \left( \E \abs{X}^p \right)^{1 / p} \leq \left( \E \abs{X}^q \right)^{1 / q}.
  \]
\end{prop}

\begin{proof}
  Define $Y = \abs{X}^p$, then we want to show that 
  $\E Y \leq (\E Y^{q / p})^{p / q}$. Note that $t \mapsto \abs{t}^{q / p}$
  is convex. Therefore, by Jensen's inequality, 
  \[
  \left( \E Y \right)^{q / p}  \leq \E \left( Y^{q / p} \right).
  \]

\end{proof}

Now we want to show that moments only depends on the distribution of the 
random variable, and it does not carry unnecessary information about the 
underlying probability space. To do this, we first show the following 
proposition.

\begin{prop}
  Let $g$ be measurable and $X$ a random variable. Then, 
  \[
  \E \abs{g(X)} = \int_\R \abs{g(t)} \d \mu_X (t).
  \]
  Moreover, if $\E \abs{g(X)} < \infty$, then 
  \[
  \E g(X) = \int_\R g(t) \d \mu_X (t).
  \]
\end{prop}

\begin{cor}
  If $\E X$ is well-defined, then 
  \[
    \E X = \int_\R t \d \mu_X (t).
  \]
  Moreover, if $X$ has distribution density $\rho_X$, then
  $\E X = \int_\R t \rho_X (t) \d t$.
\end{cor}

\begin{prop}
  If $X \geq 0$, then
  \[
  \E X = \int_0^\infty \Pr \left( X \geq t \right) \d t.
  \]
  In particular, if $X$ is non-negative and integer valued, then
  $\E X = \suminfi \Pr \left( X \geq i \right)$.
\end{prop}

\begin{proof}
  With Fubini-Tonelli, we have
  \[
  \E X = \int_0^\infty s \d \mu_X (s) 
  = \int_0^\infty \int_0^s 1 \d t \d \mu_X (s)
  = \int_0^\infty \int_t^\infty 1 \d \mu_X \d t 
  = \int_0^\infty \Pr \left( X \geq t \right) \d t.
  \]

\end{proof}

\begin{prop}
  Let $X_1, \dots, X_n$ are random variables on $(\Omega, \Sigma, \Pr)$.
  Suppose they are either all non-negative or all integrable. 
  Suppose also that they are mutually independent. Then, 
  \[
  \E \left( \prod_{i=1}^n X_i \right) = \prod_{i=1}^n \E X_i.
  \]
\end{prop}

\begin{proof}
  It suffices to show the statement for $n = 2$. Define 
  independent variables $\tilde{X_1}$ and $\tilde{X_2}$ 
  on the product space $(\R^2, \bcal_{\R^2}, \mu_{X_1} \times \mu_{X_2})$
  by coordinate projection. We know $\tilde{X_i}$ is equidistributed 
  with $X_i$, so 
  \[
  \E [X_1 X_2] = \E [\tilde{X_1} \tilde{X_2}] = \E [\tilde{X_1} \E 
  \tilde{X_2}] = \E [X_1 \E X_2],
  \]
  where in the second equality we used Fubini-Tonelli.

\end{proof}

\begin{remark}
  Recall that whenever $\E X$ is finite, $\var X = \E (X - \E X)^2
  = \E X^2 - (\E X)^2$. If $X_1, \dots, X_n$ are pairwise independent
  and have well-defined variances, then 
  \[
  \var (X_1 + \dots + X_n) = \sumi^n \var X_i.
  \]
\end{remark}

\begin{defi}[moment generating function]
  Let $X$ be a random variable, and $\lambda \in \R$. Define the 
  moment generating function of $X$ via 
  \[
  M_X (\lambda) = \E \exp (\lambda X).
  \] 
\end{defi}

Suppose $M_X (\lambda)$ is finite in some neighborhood of $0$. Then,
\[
M_X (\lambda) 
= \E \left[ \suminfn \frac{(\lambda x)^n}{n!} \right]
= \suminfn \frac{\lambda^n \E X^n}{n!}.
\]
It follows that $M_X' (0) = \E X$. Now to justify the exchang of 
integral and summation, note that 
\[
S_N = \sumi^N \frac{(\lambda x)^n}{n!} \to e^{\lambda x},
\]
and 
\[
\abs{S_N} \leq \sumi^N \frac{\abs{\lambda x}^n}{n!} 
\leq e^{\abs{\lambda x}}.
\]
However, $e^{\abs{\lambda X}} \leq e^{\lambda X} + e^{- \lambda X}$.
The expectation of RHS is finite for small $\lambda$ by assumption,
so the claim follows from dominated convergence theorem.

\begin{prop}
  Let $X$ be a non-negative random variable. Then TFAE:
  \begin{enumerate}
    \item $M_X$ is finite in some neighborhood of $0$.
    \item $\limsup_{p \to \infty} p^{-1} (\E X^p)^{1 / p} < \infty$.
  \end{enumerate}
\end{prop}

\begin{proof}
  (1) $\implies$ (2). Suppose $M_X (\epsilon) = \E \left[ \suminfn \frac{\epsilon^n 
  X_n}{n!} \right] < \infty$. This implies that $\sup_{n \geq 1} \E \left[ \frac{\epsilon^n X^n}{n!} \right] < \infty$.
  Let $1 \leq C < \infty$ be such that $\E \left[ \frac{\epsilon^n X^n}{n!} \right] < C$
  for all $n \geq 1$. We then have 
  \[
  \left( \frac{\epsilon}{n!} \right)^{1 / n} \left( \E X^n  \right)^{1 / n}
  \leq C^{1 / n}
  \] 
  It follows from Sterling's formula $n! \sim \left( n / e \right)^n$
  that 
  \[
  \frac{1}{n} \left( \E X^n \right)^{1 / n} \leq C
  \]
  for some other constant $C$.

  (2) $\implies$ (1). Suppose $\limsup_{p \to \infty} p^{-1} \left( \E X^p \right)
  ^{1 / p} \leq C < \infty$. It follows that for any $n \geq 1$
  \[
  \frac{1}{n!} \left( \E X^n \right) \leq C^n \frac{n^n}{n!} \leq C^n
  \]
  for some other constant $C$.
  Take $\epsilon = \frac{1}{2 C}$, we then have 
  \[
  \E \left[ \frac{\epsilon^n X^n}{n!} \right] \leq 2^{-n}.
  \]
  Therefore, $M_X (\epsilon) 
  = \E \left[ \suminfn \frac{\epsilon^n X^n}{n!} \right] < \infty$.

\end{proof}

Now we present an example of moment method for approximating random 
variables.
\begin{eg}
  Let $G \sim G(n, \frac{1}{2})$ be the Erdos-Renyi random graph. The goal 
  is to estimate the number of triangles in $G$. Let $N$ be 
  the number of triangles in $G$. We have 
  \[
  N = \sum_{\substack{S \subset [n] \\ \abs{S} = 3}} b_S,
  \]
  where $b_S$ is the indicator that $S$ is a triangle in $G$.
  It follows that $\E N = \frac{1}{8} \binom{n}{3}$. Also, 
  \[
  \E N^2 = \sum_{\abs{S} = 3} \sum_{\abs{S'} = 3} \E [b_S b_{S'}]
  \]
  To compute this, we consider several cases. 
  \begin{enumerate}
    \item If $S \cap S' = \emptyset$, then $\E [b_S b_{S'}] = \frac{1}{64}$.
    \item If $\abs{S \cap S'} = 1$, then $\E [b_S b_{S'}] = \frac{1}{64}$.
    \item If $\abs{S \cap S'} = 3$, then $\E [b_S b_{S'}] = \frac{1}{8}$.
    \item If $\abs{S \cap S'} = 2$, then $\E [b_S b_{S'}] = \frac{1}{32}$.
  \end{enumerate}
  Hence, 
  \[
  \E N^2 = \frac{1}{64} \binom{n}{3}^2 \pm O(n^4),
  \]
  where the $O(n^4)$ term comes from the cases where $\abs{S \cap S'} = 2$
  or $3$. Therefore, 
  \[
  \var N = \E N^2 - (\E N)^2 = O(n^4).
  \]
  Using Chebyshev's inequality, for each $t > 0$ we have
  \[
  \Pr \left( \abs{N - \E N} \geq t \cdot \E N \right)
  \leq \frac{\var N}{t^2 (\E N)^2} = t^{-2} O(n^{-2})
  \to 0.
  \]
  as $n \to \infty$.

\end{eg}

We have the following proposition on sub-Gaussian decay of moments.
\begin{prop}
  Let $X$ be a random variable, TFAE:
  \begin{enumerate}
    \item $\sup_{p \geq 1} (\E \abs{X}^p)^{1 / p} p^{-1/2} < \infty$.
    \item there exists $c > 0$ such that $\Pr \left( \abs{X} > t \right)
    \leq 2 \exp (-c t^2)$ for all $t > 0$.
  \end{enumerate}
  If any of these statement is satisfied, $X$ is called a \emph{subgaussian 
  variable}.
\end{prop}

\begin{proof}
  Exercise.

\end{proof}

\begin{thm}[Khintchine's inequality]
  There exists a universal constant $C < \infty$ such that for any
  $n \in \N$, $a_1, \dots, a_n \in \R$, and $p \geq 1$, we have 
  \[
  \left( \E \abs{\sumi^n a_i r_i}^p \right)^{1 / p} \leq 
  C \sqrt{p} \norm{a}_2,
  \]
  where $r_1, \dots, r_n$ are i.i.d.\ Rademacher variables:
  $\Pr \left( r_i = 1 \right) = \Pr \left( r_i = -1 \right)
  = \frac{1}{2}$.
\end{thm}

\begin{proof}
  WLOG assume $\norm{a}_2= 1$. Let $\lambda > 0$. For any $t > 0$, we have
  \begin{align*}
    \Pr \left( \sumi^n a_i r_i > t \right)
    = \Pr \left( \exp \left( \lambda \sumi^n a_i r_i \right)
    > \exp (\lambda t) \right)
    \leq \frac{\E \exp \left( \lambda \sumi^n a_i r_i \right)}
    {\exp (\lambda t)}
  \end{align*}
  from Markov's inequality. By independence, we have 
  \begin{align*}
    \Pr \left( \sumi^n a_i r_i > t \right)
    \leq \frac{\prod_{i=1}^{n} \E \exp (\lambda a_i r_i) }{\exp (\lambda t)}
    = \frac{\prod_{i=1}^n \frac{1}{2} \left( \exp (\lambda a_i) + 
    \exp (- \lambda a_i) \right) }{\exp (\lambda t)}.
  \end{align*}
  Now we use $\exp (x) \leq x + \exp (x^2)$ to obtain
  \begin{align*}
    \Pr \left( \sumi^n a_i r_i > t \right)
    \leq \frac{\prod_{i=1}^n \exp (\lambda^2 a_i^2)}{\exp (\lambda t)} 
    = \exp \left( \lambda^2 \norm{a}^2_2 - \lambda t \right).
  \end{align*}
  This holds for any $\lambda > 0$. Substituting $\lambda = 
  \frac{1}{2} t \norm{a}^{-2}_2$, we have 
  \[
  \Pr \left( \sumi^n a_i r_i > t \right) \leq \exp \left( -\frac{1}{4} 
  \frac{t^2}{\norm{a}^2_2} \right)
  \]
  Similarly we can bound $\Pr \left( \sumi^n a_i r_i < -t \right)$
  as the Rademacher variables are symmetric. Hence we have 
  \[
  \Pr \left( \abs{\sumi^n a_i r_i} > t \right) \leq 2
  \exp \left( -\frac{1}{4} t^2 \right).
  \]
  The theorem follows from the previous proposition.

\end{proof}

\begin{thm}[Anti-concentration inequalities]
  Let $X$ be a non-negative random variable and $1 \leq p < q < \infty$.
  Suppose $\left( \E X^p \right)^{1 / p} \leq C \left( \E X^q \right)^{1 / q}$
  for some constant $C$. Then there exsits $\epsilon > 0$ depending 
  only on $p$, $q$, $C$ such that 
  \[
  \Pr \left( X \geq \epsilon \left( \E X^p \right)^{1 / p} \right) 
  \geq \epsilon.
  \]
\end{thm}

\subsection{Convergence of random variable}

\begin{defi}
  Let $\seqinfn{X_n}$ be a sequence of random variables and $X$ is also 
  a random variable.
  Say $X_n$ converges to $X$ \emph{in distribution} or \emph{weakly} if 
  for any compactly supported continuous function $f$, we have 
  \[
  \int_\R f(t) \d \mu_{X_n} (t) \to \int_\R f(t) \d \mu_X (t).
  \]
  Write $X_n \convd X$ or $X_n \convw X$.
\end{defi}

\begin{prop}
  $X_n \convw X$ iff $F_{X_n} (t) \to F_X (t)$ at every point $t$ 
  where $F_X$ is continuous.
\end{prop}

\begin{defi}
  Let $\seqinfn{X_n}$ be a sequence of random variables and $X$ is also 
  a random variable defined on the same probability space. 
  Say $X_n$ converges to $X$ \emph{in probability} if 
  for any $\epsilon > 0$, 
  \[
  \Pr \left( \abs{X_n - X} < \epsilon \right) \to 1.
  \]
  Write $X_n \convp X$.
\end{defi}

\begin{prop}
  If $X_n \convp X$, then $X_n \convd X$.
\end{prop}

\begin{proof}
  Let $t \in \R$ be such that $F_X$ is continuous at point $t$
  and let $\epsilon > 0$. Let $\delta > 0$ be such taht 
  $F_X (t + \delta) \leq F_X (t) + \epsilon$ and 
  $F_X (t - \delta) \geq F_X (t) - \epsilon$.
  Since $\Pr \left( \abs{X - X_n} < \delta\right)
  \to 1$, there exists $n_0$ such that $n \geq n_0$ implies 
  \[
  \Pr \left( \abs{X_n - X} < \delta\right)
  \geq 1 - \epsilon.
  \]
  Take $n \geq n_0$. We have 
  \begin{align*}
    F_{X_n} (t) 
    &= \Pr \left( X_n \leq t \right) \\
    &\leq \Pr \left( \abs{X_n - X} > \delta\right)
    + \Pr \left( X \leq t + \delta\right) \\
    &\leq \epsilon + F_X (t) + \epsilon \\
    &\leq F_X (t) + 2 \epsilon.
  \end{align*}
  Similarly for the other side.

\end{proof}

\begin{prop}
  If $X_n \to X$ a.s., then $X_n \convp X$.
\end{prop}

\begin{proof}
  Let $\epsilon > 0$. For each $\omega \in \Omega$, define 
  \[
  n_\epsilon (\omega) = \min \left\{ n : \abs{X_m(\omega) - X(\omega)} 
  \leq \epsilon \text{ for all $m \geq n$} \right\} < \infty.
  \]
  By continuity of measure, for any $\delta > 0$ there exists $N$ such that 
  $\Pr \left( n_\epsilon \leq N \right) \geq 1 - \delta$.
  This implies that for all $m \geq N$, we have 
  \[
  \Pr \left( \abs{X_m - X} \leq \epsilon \right) \leq 1 - \delta.
  \]
  Since $\delta > 0$ is arbitrary, this compeltes the proof.

\end{proof}

\begin{prop}
  Let $c \in \R$ and suppose $\seqinfn{X_n}$ are random variables 
  on a common probability space 
  $(\Omega, \Sigma, \Pr)$. Then $X_n \convd c$ iff $X_n \convp c$.
\end{prop}

\begin{proof}
  Just need to check that $X_n \convd c$ implies $X_n \convp c$.
  We have 
  \[
  F_c (t) = \begin{cases}
    1 & \text{if $t \geq c$,} \\
    0 & \text{otherwise.}
  \end{cases}
  \]
  Take $\epsilon > 0$, then $c \pm \epsilon$ are points of continuity.
  It follows that $F_{X_n} (c + \epsilon) \to 1$ and $F_{X_n} (c - \epsilon)
  \to 0$. This implies that 
  \[
  \Pr \left( \abs{X_n - c} > \epsilon \right) \to 0.
  \]

\end{proof}


Next we consider algebraic operations and convergence.
It is clear that if $X_n \to X$ and $Y_n \to Y$ a.e.\, then $X_n + Y_n 
\to X + Y$ a.e.\ Also, if $X_n \convp X$ and $Y_n \convp Y$, then 
$X_n + Y_n \convp X + Y$. But for convergence in distribution, 
the same statement need not be true without any extra assumptions.
However, suppose $X_n$ and $Y_n$ are independent for each $n$, $X$ and $Y$
are independent, $X_n \convd X$, and $Y_n \convd Y$. Then $X_n + Y_n 
\convd X + Y$.


\begin{eg}
  Suppose $\seqinfi{a_i}$ are i.i.d.\ random variables and 
  \[
  \epsilon_n \sumi^n a_i \convd X
  \]
  for some random variable $X$ and sequence of positive 
  numbers $\seqinfn{\epsilon_n}$.
  Then we have 
  \[
  \epsilon_{2n} \sumi^{2n} a_i \convd X,
  \]
  and 
  \[
  \frac{\epsilon_{2n}}{\epsilon_n} \left( \epsilon_n \sumi^n a_i \right)
  + \frac{\epsilon_{2n}}{\epsilon_n} \left( \epsilon_n \sum_{i=n+1}^{2n} a_i \right)
  \convd X.
  \]
  If we further suppose that there is $\alpha > 0$ such that 
  \[
  \lim_{n \to \infty} \frac{\epsilon_{2n}}{\epsilon_n} = \alpha,
  \]
  we would have $\alpha X + \alpha \tilde{X} \sim X$, where 
  $\tilde{X}$ is an independent copy of $X$. The only such distribution 
  $X$ with bounded second moment is the Gaussian distribution.

\end{eg}

Recall that for independent random variables, we have a representation 
theorem that create new independent random variables on the product space.
We alos have a representation theorem for random variables converging 
in distribution. On the new probability space, they will coneverge a.e.\

\begin{thm}[Skorokhod's representation theorem]
  Suppose $X_n \convd X$ on some probability space $(\Omega, \Sigma, \Pr)$.
  Then there exists another probability space $(\tilde{\Omega}, 
  \tilde{\Sigma}, \tilde{\Pr})$ and random variables $\tilde{X}_n$, 
  $\tilde{X}$ on it such that the following holds:
  \begin{itemize}
    \item $\tilde{X}_n \sim X_n$ for all $n$.
    \item $\tilde{X} \sim X$.
    \item $\tilde{X}_n \to \tilde{X}$ a.e.\
  \end{itemize}
\end{thm}

\begin{proof}
  Suppose for simplicity that $F_{X_n}$ and $F_X$ are all continuous and 
  strictly increasing. With this, we can easily construct random 
  variables with the same distribution. Let $U \sim \unif([0, 1])$,
  then $F_X^{-1} (U) \sim X$. Indeed, 
  \[
  \Pr \left( F_X^{-1} (U) \leq t \right)
  = \Pr \left( F_X (F_X^{-1} (U)) \leq F_X (t) \right)
  = F_X (t).
  \] 

  Take $([0, 1], \bcal_{[0, 1]}, m)$. 
  Let $\tilde{X}_n = F_{X_n}^{-1} (\omega)$
  and $\tilde{X} = F_X^{-1} (\omega)$. Then 
  $\tilde{X}_n \sim X_n$ and $\tilde{X} \sim X$.
  Note that $X_n \convd X$ 
  implies $F_{X_n} \to F_X$ pointwise. Checking
  $\tilde{X}_n \to \tilde{X}$ a.e.\ is left as an exercise.

\end{proof}

\subsection{Law of rare events}

As an example for convergence of random variables, we have the following
example presenting the law of rare events. First we need some definitions.

\begin{defi}[Poisson distribution]
  Say random variable $X \sim \Poisson (\lambda)$ has Possion distribution 
  with parameter $\lambda$ for $\lambda > 0$ if 
  \[
  \Pr \left( X = m \right) = \frac{\lambda^m e^{- \lambda}}{m!}.
  \]
  Then, $\E X = \lambda$, $\E X^2 = \lambda^2 + \lambda$, and 
  $\var X = \lambda$.
\end{defi}

\begin{defi}[exponential distribution]
  Say random variable $X \sim \Exp (\lambda)$ has exponential distribution
  with parameter $\lambda$ if $X$ has density function 
  \[
  \rho_X (t) = \lambda \exp (- \lambda t) \quad \text{for $t \geq 0$.}
  \]
  Then the corresponding CDF is 
  \[
  F_X (t) = \begin{cases}
    1 - \exp (- \lambda t), & \text{ if $t \geq 0$,} \\
    0, & \text{ otherwise.}
  \end{cases}
  \]
\end{defi}

The exponential distribution is a memoryless distribution: suppose 
$X \sim \Exp (\lambda)$ and $t > s > 0$. Then, 
\[
\Pr \left( X \geq t \mid X \geq s \right)
= \frac{\Pr \left( X \geq t \right)}{\Pr \left( X \geq s \right)}
= \exp (- \lambda (t - s)) 
= \Pr \left( X \geq t - s \right).
\]

\begin{eg}[law of rare events]
  Let $\lambda > 0$ be a fixed parameter. Let $\seqinfn{m_n}$ 
  be a non-decreasing sequence of integers such that 
  \[
  \lim_{n \to \infty} \frac{m_n}{n} = \lambda.
  \]
  Create a triangular array as follows: for any $n$, 
  let $\left\{ U_{n, j} \right\}_{j=1}^{m_n}$ be i.i.d.\ uniform random 
  variables on $[0, n]$. Now let
  \[
  X_n = \abs{ \left\{ i \leq m_n : U_{n, i} \in [0, 1] \right\} }.
  \]
  Then $X_n \convd \Poisson (\lambda)$.

  To prove this, let $k \geq 0$, we have
  \begin{align*}
    \Pr (X_n = k) 
    &= \binom{m_n}{k} \left( \frac{1}{n} \right)^k 
    \left( 1 - \frac{1}{n} \right)^{m_n - k} \\
    &= (1 + o(1)) \frac{m_n^k}{k!} n^{-k} \exp \left( - \frac{m_n}{n} \right) \\
    &\to (1 + o(1)) \frac{\lambda^k e^{- \lambda}}{k!}
  \end{align*}
  as $n \to \infty$. Therefore, $X_n \convd \Poisson (\lambda)$ by the 
  follwoing proposition.

\end{eg}

\begin{prop}
  Let $\seqinfn{X_n}$ and $X$ be integer-valued random variables.
  Then $X_n \convd X$ iff $f_{X_n} (k) \to f_X (k)$ for any $k \in \Z$.
\end{prop}

\begin{proof}
  Verify directly using compactly supported continuous test functions. 
  Since the test functions 
  are compactly supported, only finite sums are involved.

\end{proof}

\begin{eg}
  Continuing the previous example, define 
  \[
  Y_n = \min_{1 \leq i \leq m_n} U_{n, i}.
  \]
  Then $Y_n \convd \Exp (\lambda)$.

  Let $t > 0$, then 
  \begin{align*}
    \Pr \left( Y_n \leq t \right)
    &= 1 - \Pr \left( Y_n > t \right) \\
    &= 1 - \Pr \left( \bigcap_{i=1}^{m_n} U_{n, i} > t \right) \\
    &= 1 - \Pr \left( U_{n, 1} > t \right)^{m_n} \\
    &= 1 - \left( \frac{n - t}{n} \right)^{m_n}  \\
    &\to 1 - \exp (- \lambda t)
  \end{align*}
  as $n \to \infty$.

\end{eg}

\begin{eg}[Poisson process]
  Fix integer parameter $w > 0$. For every large $n$, let 
  \[
  Z_n = (U_{n, 1}^*, U_{n, 2}^*, \dots, U_{n, w}^*)
  \]
  be $w$ smallest numbers from the set $\left\{ U_{n, i} \right\}_{i=1}^{m_n}$,
  arranged in increasing order, then $Z_n$ is a random 
  vector in $\R^w$. We want to investigate the limiting distribution 
  of $\seqinfn{Z_n}$, if any exists.

  \textbf{Claim:} let $\xi_1$, \dots, $\xi_w \sim \Exp (\lambda)$ be i.i.d.\
  Let the random vector 
  \[
  \xi = (\xi_1, \xi_1 + \xi_2, \dots, \xi_1 + \dots + \xi_w).
  \]
  Then $Z_n \convd \xi$.

\end{eg}

\begin{eg}[Poisson process]
  Let $\seqinfn{X_n}$ be a sequence of i.i.d.\ 
  random variables that follows the exponential distribution 
  $\Exp (\lambda)$. Define Poisson process $\{S_n\}_{n=0}^\infty$ via 
  \[
  S_n = \sumi^n X_i.
  \]
  This has 
  two properties:
  \begin{enumerate}
    \item For each interval of length $1$ on $\R_+$, the number of 
    $S_n$'s within this interval follows the Poission distribution 
    with parameter $\lambda$.

    \item For any finite collection of disjoint intervals, the variables
    corresponding to the number of $S_n$'s within the intervals are 
    mutually independent. This is because of the memoryless property 
    of the exponential distribution.
  \end{enumerate}
\end{eg}

\begin{lemma}
  If $c_j \to 0$, $a_j \to \infty$ and $a_j c_j \to \lambda$, 
  then $(1 + c_j)^{a_j} \to e^{\lambda}$.
\end{lemma}

\begin{thm}[Central limit theorem, basic form]
  Let $\seqinfn{b_n}$ be i.i.d.\ random variables that are 
  $\Bernoulli (\frac{1}{2})$. Define $S_n = \sumi b_i$.
  Then 
  \[
  \frac{S_n - \frac{n}{2}}{\sqrt{n} / 2} \convd \ncal (0, 1).
  \]
\end{thm}

\begin{proof}
  Suppose $n$ and $k$ be some integer, then 
  \begin{align*}
    \Pr \left( S_{2n} = n + k \right)
    &= \frac{(2n) !}{(n + k) ! (n - k)!} 2^{-2n}
  \end{align*}
  Using Sterling's formula $n! \sim \sqrt{2 \pi n} (\frac{n}{e})^n$, 
  we have 
  \begin{align*}
    \frac{(2n) !}{(n + k) ! (n - k)!}
    &\sim \frac{ (2n)^{2n} }{(n + k)^{n + k} (n - k)^{n - k}} 
    \frac{\sqrt{2 \pi n}}{\sqrt{2 \pi (n + k)} \sqrt{2 \pi (n - k)}}
  \end{align*}
  Hence,
  \begin{align*}
    \Pr \left( S_n = n + k \right)
    &\sim \left( 1 + \frac{k}{n} \right)^{- n - k - \frac{1}{2}} 
    \left( 1 - \frac{k}{n} \right)^{-n + k - \frac{1}{2}} 
    \left( \pi n \right)^{\frac{1}{2}}.
  \end{align*}
  We also have 
  \[
  \left( 1 + \frac{k}{n} \right)^{- n - k} 
  \left( 1 - \frac{k}{n} \right)^{-n + k} 
  = \left( 1 - \frac{k^2}{n^2} \right)^{-n} 
  \left( 1 + \frac{k}{n} \right)^{-k} 
  \left( 1 - \frac{k}{n} \right)^{k}.
  \]
  Letting $k = x \sqrt{n / 2}$, we know
  \begin{align*}
    \left( 1 - \frac{k^2}{n^2} \right)^{-n} 
    \left( 1 + \frac{k}{n} \right)^{-k} 
    \left( 1 + \frac{k}{n} \right)^{k}
    &= \left( 1 - \frac{x^2}{2 n} \right)^{-n} 
    \left( 1 + \frac{x}{\sqrt{2n}} \right)^{- x \sqrt{n/2}}
    \left( 1 - \frac{x}{\sqrt{2n}} \right)^{- x \sqrt{n/2}} \\
    &\to \exp \left( \frac{x^2}{2} \right) 
    \exp \left( - \frac{x^2}{2} \right) 
    \exp \left( - \frac{x^2}{2} \right) \\
    &= \exp \left( - \frac{x^2}{2} \right)
  \end{align*}
  by the lemma above.

  Therefore, if $2k / \sqrt{2n} \to x$, then 
  \[
  \Pr \left( S_{2n} = n + k \right)
  \sim \left( \pi n \right)^{- 1 / 2} e^{- x^2 / 2}.
  \]
  Now, 
  \begin{align*}
    \Pr \left( a \leq \frac{S_{2n} - n}{ \sqrt{n / 2} } \leq b \right)
   &= \Pr \left( n + a \sqrt{n / 2} \leq S_{2n} \leq n + b \sqrt{n / 2} \right) \\
   &= \sum_{k \in \left[ a \sqrt{n / 2}, b \sqrt{n / 2} \right] \cap \Z}
   \Pr \left( S_{2n} = n + k \right).
  \end{align*}
  Chaging variables $x = k \sqrt{n / 2}$, we then have 
  \begin{align*}
    \Pr \left( a \leq \frac{S_{2n} - n}{\sqrt{n / 2}} \leq b \right) 
    &= \sum_{x \in [a, b] \cap \left( 2\Z / \sqrt{2n} \right)} 
    (2 \pi)^{- 1 / 2} e^{-x^2 / 2} (2 / n)^{1 / 2} \\
    &\approx \int_a^b \frac{1}{\sqrt{2 \pi}} e^{-x^2 / 2} \d x,
  \end{align*}
  using the definition of Riemann integral. The proof is  
  now complete.

\end{proof}

\begin{defi}[Multivariate Gaussian]
  There are two equivalent definitions of multivariate Gaussian.
  \begin{enumerate}
    \item $X \in \R^n$ has Gaussian distribution if there is non-random 
    $n$-by-$n$ non-random matrix and $\mu \in \R^n$ such that 
    $X \sim M G + \mu$, where $G$ is the standard Gaussian vector in $\R^n$.
    That is, $G = (G_1, \dots, G_n)$ are random vector with 
    i.i.d.\ $\ncal(0, 1)$ components. 

    \item $X \in \R^n$ has Gaussian distribution if for all 
    non-random $y \in \R^n$, $\bra X, y \ket$ is a scalar 
    Gaussian random variables.
  \end{enumerate}
\end{defi}

\begin{proof}
  (1) $\implies$ (2). Exercise. \TODO

  (2) $\implies$ (1). Later using characteristic functions.

\end{proof}

\section{Law of large numbers}

\subsection{Weak law of large numbers}

\begin{thm}[Weak law of large numbers]
  Suppose $\seqinfn{X_n}$ are mutually independent, identically 
  distributed random variables. Suppose also $\E \abs{X_1} < \infty$.
  Let $S_n = \suminfn X_i$, then $S_n / n \convp \E X_1$.
\end{thm}

\begin{proof}
  We prove this using truncation method.
  Let $\epsilon > 0$ and choose truncation level $M (\epsilon) > 0$ 
  satisfying the following:
  \begin{align*}
    \E \abs{X_1 \ind_{\abs{X_1} > M}} < \epsilon^2.
  \end{align*}
  By dominated convergence theorem, there exists such an $M$. 
  Note that this also implies 
  \[
  \abs{\E X_1 - \E \left( X_1 \ind_{\abs{X_1} \leq M} \right)} < \epsilon.
  \]
  For each $n$, we have 
  \[
  S_n = \sumk^n X_k \ind_{\abs{X_k} \leq M} + 
  \sumk^n X_k \ind_{\abs{X_k} > M}.
  \]
  Write
  \[
  T_n = \sumk^n X_k \ind_{\abs{X_k} \leq M} \quad
  \text{and} \quad
  R_n = \sumk^n X_k \ind_{\abs{X_k} > M}.
  \]
  For the first term, we can use Chebyshev's inequality to obtain
  \begin{align*}
    \Pr \left( \abs{\frac{T_n}{n} - 
    \E [X_1 \ind_{\abs{X_1} \leq M}]} \geq \epsilon \right)  
    &\leq \frac{1}{\epsilon^2} \var \left( \frac{T_n}{n} \right)\\
    &= \frac{1}{n \epsilon^2} \var (X_1 \ind_{\abs{X_1} \leq M})  \\
    &\leq \frac{4 M^2}{n \epsilon^2}
  \end{align*}
  Similarly, for the second term, we have 
  \begin{align*}
    \Pr \left( \abs{\frac{R_n}{n}} \geq \epsilon \right) 
    &\leq \frac{1}{\epsilon} \E \abs{\frac{R_n}{n}} \leq \epsilon.
  \end{align*}
  It follows that 
  \[
  \Pr \left( \abs{\frac{S_n}{n} - \E X_1} \geq 
  2 \epsilon + \epsilon^2 \right) 
  \leq \frac{4 M^2}{n \epsilon^2} + \epsilon.
  \]
\end{proof}

\begin{remark}
  Note that the only step we use independence is when we calculate 
  $\var (T_n / n)$. In fact, pairwise independence is sufficient 
  for this step.
\end{remark}

\subsection{Borel-Cantelli lemmas}

\begin{thm}[Borel-Cantelli lemmas]
  Let $\seqinfn{A_n}$ be a sequence of events and 
  \[
  \Pr \left( A_n \text{ i.o.} \right)
  = \Pr \left( \capinfk \bigcup_{n=k}^\infty A_n \right)
  = \Pr \left( \limsup A_n \right)
  \]
  \begin{enumerate}
    \item If $\suminfn \Pr (A_n) < \infty$, then 
    $\Pr \left( A_n \text{ i.o.} \right) = 0$.

    \item Suppose also $A_n$'s are mutually independent
    and $\suminfn \Pr (A_n) = \infty$, then 
    $\Pr \left( A_n \text{ i.o.} \right) = 1$.
  \end{enumerate}
\end{thm}

\begin{proof}
\begin{enumerate}
  \item We have 
  \begin{align*}
    \Pr \left( A_n \text{ i.o.} \right) 
    &= \Pr \left( \suminfn \ind_{A_n} = \infty \right).
  \end{align*}
  However, $\E [\suminfn \ind_{A_n}] = \suminfn \Pr (A_n) < \infty$,
  so $\Pr (A_n \text{ i.o.}) = 0$.

  \item We have 
  \begin{align*}
    \Pr \left( A_n \text{ i.o.} \right) 
    = \Pr \left( \cupinfk \bigcap_{n = k}^\infty A_n^c \right)
    \leq \suminfk \Pr \left( \bigcap_{n=k}^\infty A_n^c \right).
  \end{align*}
  Using independence and $1 - t \leq \exp (- t)$, 
  we obtain 
  \[
  \Pr \left( \bigcap_{n=k}^M A_n \right)
  = \prod_{n=k}^M (1 - \Pr (A_n)) 
  \leq \exp \left( - \sum_{n=k}^M \Pr (A_n) \right) \to 0
  \]
  as $M \to \infty$. Hence, $\Pr \left( \bigcup_{n=k}^\infty A_n \right)
  = 1$ for all $k$. Since $\bigcup_{n=k}^\infty A_n \downarrow \limsup A_n$,
  it follows from monotone continuity that $\Pr (A_n \text{ i.o.}) = 1$.
\end{enumerate}
\end{proof}

\begin{prop}
  Let $\seqinfn{b_n}$ be mutually independent Bernoulli variables 
  with parameters $\seqinfn{p_n}$ with $0 < p_n < 1$. 
  Then, $b_n \to 0$ a.e.\ iff $\suminfn p_n < \infty$.
\end{prop}

\begin{proof}
  For any $\omega$, $b_n (\omega) \to 0$ as $n \to \infty$ iff 
  $b_n (\omega) = 1$ for at most finitely many $n$'s. 
  Let $A_n = \left\{ b_n = 1 \right\}$. Then, 
  $b_n \to 0$ a.e.\ iff for a.e.\ $\omega$ is contained 
  in finitely many $A_n$'s. That is, $\Pr (A_n \text{ i.o.}) = 0$.
  The proposition then follows from Borel-Cantelli lemma.

\end{proof}

\subsection{Strong law of large numbers}

\begin{thm}
  Let $\seqinfn{X_n}$ be mutually independent identically distributed
  random variables. Suppose $\E \abs{X_1} < \infty$
  and let $S_n = \frac{1}{n} \sumk^n X_k$ 
  Then, $S_n \to \E X_1$ a.e.\
\end{thm}

\begin{proof}
  See notes for proof by Etemadi.
\end{proof}

\begin{eg}[Coupon collector]
  Consider array of random variable $\seqinfn{\seqinfj{X_{j, n}}}$. 
  For each $n$, the sequence $\seqinfj{X_{j, n}}$
  are i.i.d.\ uniform on $[n] = \left\{ 1, \dots, n \right\}$.
  Let 
  \[
  T_n = \min \left\{ m \geq 1 : \cupi^m \{X_{i, n}\} = [n] \right\}.
  \]
  We want to show $T_n / (n \log n) \convp 1$.

  To show this, we use second moment method.
  For each $k \leq n$, let $T_{n, k}$ be the first time $k$ coupons 
  are collected. We then have $T_{n, 0} = 0$ and 
  \[
  T_n = \sumk^n (T_{n, k} - T_{n, k - 1}).
  \]
  Note that $T_{n, k} - T_{n, k - 1}$ follows a geometric distribution.
  Hence,
  \begin{align*}
    &\E \left( T_{n, k} - T_{n, k - 1} \right) = \frac{n}{n - k + 1}, \\
    &\var \left( T_{n, k} - T_{n, k - 1} \right) =  
    \frac{k - 1}{n} \frac{1}{\left( 1 - \frac{k - 1}{n} \right)^2}.
  \end{align*}
  It follows that 
  \[
  \E T_n = n \sumk^n \frac{1}{n - k + 1} \sim n \log n.
  \]
  Also, 
  \begin{align*}
    \var T_n 
    &= \sumk^n \var \left( T_{n, k} - T_{n, k - 1} \right)
    = \sumk^n \frac{k - 1}{n} \frac{1}{\left( 1 - \frac{k - 1}{n} \right)^2}
    = o(n^2 \log^2 n)
  \end{align*}
  It then follows from Chebyshev's inequality that 
  $T_n / (n \log n) \convp 1$.

\end{eg}

\begin{eg}
  Let $v_0 \in \left\{ -1, 1 \right\}^n$ and define a sequence 
  of random vectors $\seqinfi{X_i}$ inductively as follows:
  define $X_0 = v_0$, and $X_{i + 1}$ is a random vector supported 
  on $\left\{ -1, 1 \right\}^n$ obtained from $X_i$ by selecting 
  a random nuber $m \in [n]$ uniformly at random and flipping 
  $m$-th component of $X_i$ with probability $\frac{1}{2}$.

  Now $X_k$ is a random vector for each $k$. 
  Denote the uniform distribution on $\left\{ -1, 1 \right\}^n$ 
  as $P$, and we are interested in 
  \[
  \norm{X_k - P}_{TV} 
  = \inf_{\xi, \tilde{X}} \Pr \left( \xi \neq \tilde{X} \right),
  \]
  where the inf is taken over all pairs of random vectors 
  $\xi$ and $\tilde{X}$ such that $\tilde{X} \sim X_k$ and 
  $\xi \sim P$.

  \textbf{Claim:} to make sure that $\norm{X_k - P}_{TV} = o(1)$,
  we must have $k = \Omega (n \log n)$. 
  
  This is similar to the coupon collector problem. We need to have selected 
  at least $n (1 - O( n^{-1/2} ))$ indicies for the distribution 
  of $X_k$ to be roughly uniform. With a similar derivation 
  to the coupon collector problem, this implies $k = \Omega (n \log n)$.


\end{eg}

\section{Central Limit theorem}

\subsection{Characteristic Function}

\begin{defi}[characteristic function]
  Let $X$ be a random vector in $\R^n$, we define the 
  characteristic function of $X : \R^n \to \C$ via 
  \[
  \phi_X (t) = \E e^{i \bra X, t \ket}
  \]
  Suppose $X$ has density $\rho_X$, then 
  \[
  \phi_X (t) = \int_{\R^n} \exp \left( i \bra s, t \ket \right) 
  \rho_X (s) \d s.
  \]
  This is essentially the Fourier transform of $\rho_X$.
\end{defi}

\begin{prop}
  Let $X$ be a random variable, its characteristic function 
  $\phi_X$ is uniformly continuous.
\end{prop}

\begin{proof}
  We have 
  \begin{align*}
    \abs{\phi_X (t + \delta) - \phi_X (t)} 
    &= \abs{ \E e^{i \bra X, t \ket} \left( e^{i \bra x, \delta \ket} - 1 \right) }
    \leq \E \abs{ e^{i \bra X, \delta \ket} - 1 }.
  \end{align*}
  Note that $\abs{e^{i \bra X, \delta \ket} - 1} \leq 2$. The proposition 
  then immediately follows from dominated convergence theorem.
\end{proof}

\begin{prop}
  If $X$ is a random variable with $\E \abs{X}^k < \infty$ 
  for some $k \geq 1$. Then $\phi_X$ is $k$ times continuous 
  differentiable and 
  \[
  \phi^{(k)}(0) = i^k \E X^k.
  \]
\end{prop}

\begin{prop}
  Suppose $X$ is random variable and $a, b \in \R$. Then 
  \[
  \phi_{aX + b} (t) = \exp (itb) \phi_X (at).
  \]
\end{prop}

\begin{prop}
  If $X_1, X_2, \dots, X_n$ are mutually independent and 
  $S = X_1 + \dots + X_n$, then 
  \[
  \phi_S (t) = \prod_{k=1}^n \phi_{X_k} (t).
  \]
\end{prop}

\begin{prop}
  Suppose $X \sim \ncal (0, 1)$, then 
  \[
  \phi_X (t) = \exp \left( - \frac{t^2}{2} \right).
  \]
\end{prop}

\begin{proof}
  Suppose $Y \sim \ncal (0, 1)$, and $X$, $Y$ are independent.
  It follows that 
  \[
  \phi_{X + Y} (t) = \phi_X (t) \phi_Y (t) = \phi_X (t)^2.
  \]
  Meanwhile,
  \begin{align*}
    \phi_{X + Y} (t) 
    &= \frac{1}{2 \pi} \int_{-\infty}^\infty \int_{-\infty}^\infty 
    \exp \left( i (x + y) t \right) 
    \exp \left( - \frac{x^2}{2} \right)
    \exp \left( - \frac{y^2}{2} \right) 
    \d x \d y
  \end{align*}
  Using change of variable with $u = x + y$, $v = x - y$, we obtain 
  \begin{align*}
    \phi_{X + Y} (t) 
    &= \frac{1}{4 \pi} \int_{-\infty}^\infty \int_{-\infty}^\infty 
    \exp \left( -\frac{1}{2} \left( \frac{u^2}{2} 
    + \frac{v^2}{2} \right) + i u t \right)
    \d u \d v \\
    &= \frac{1}{\sqrt{4 \pi}} \int \exp 
    \left( - \frac{1}{4} u^2 + i t u \right) \d u \\
    &= \phi_X (\sqrt{2} t).
  \end{align*}
  Now we have a function equation
  \[
  \phi_X (\sqrt{2} t) = \phi_X (t)^2.
  \]
  This implies that $\phi_X (t) = \exp (- t^2 / 2)$.

\end{proof}

\end{document}